<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.1" />
<title>pwlf.pwlf API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id=^header-]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:1px solid #ddd;margin:1em 0 1em 4ch}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}#index .two-column{column-count:2}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{background:#ffc;font-weight:400;font-size:.8em;width:11em;text-transform:uppercase;padding:0px 8px;border:1px solid #fd6;border-radius:5px;cursor:pointer}.source summary:hover{background:#fe9 !important}.source[open] summary{background:#fda}.source pre{max-height:500px;overflow-y:scroll;margin-bottom:15px}.hlist{list-syle:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink;]</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a,a:visited{text-decoration:underline}a[href]:after{content:" (" attr(href) ")"}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>pwlf.pwlf</code> module</h1>
</header>
<section id="section-intro">
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -- coding: utf-8 --
# MIT License
#
# Copyright (c) 2017, 2018 Charles Jekel
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the &#34;Software&#34;), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import print_function
# import libraries
import numpy as np
from scipy.optimize import differential_evolution
from scipy.optimize import fmin_l_bfgs_b
from scipy import stats
from pyDOE import lhs

# piecewise linear fit library


class PiecewiseLinFit(object):

    def __init__(self, x, y, disp_res=False, sorted_data=False):
        r&#34;&#34;&#34;
        An object to fit a continuous piecewise linear function
        to data.

        Initiate the library with the supplied x and y data.
        Supply the x and y data of which you&#39;ll be fitting
        a continuous piecewise linear model to where y(x).
        by default pwlf won&#39;t print the optimization results.;

        Parameters
        ----------
        x : array_like
            The x or independent data point locations as list or 1 dimensional
            numpy array. The x and y data should be ordered such that x[i]
            corresponds to y[i], for an arbitrary index i.
        y : array_like
            The y or dependent data point locations as list or 1 dimensional
            numpy array.
        disp_res : bool, optional
            Whether the optimization results should be printed. Default is
            False.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implementation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A process that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.

        Attributes
        --------
        x_data : ndarray (1-D)
            The inputted parameter x from the 1-D data set.
        y_data : ndarray (1-D)
            The inputted parameter y from the 1-D data set.
        n_data : int
            The number of data points.
        break_0 : float
            The smallest x value.
        break_n : float
            The largest x value.
        print : bool
            Whether the optimization results should be printed. Default is
            False.

        Methods
        -------
        fit(n_segments, x_c=None, y_c=None, **kwargs)
            Fit a continuous piecewise linear function for a specified number
            of line segments.
        fitfast(n_segments, pop=2, **kwargs)
            Fit a continuous piecewise linear function for a specified number
            of line segments using a specialized optimization routine that
            should be faster than fit() for large problems. The tradeoff may
            be that fitfast() results in a lower quality model.
        fit_with_breaks(breaks)
            Fit a continuous piecewise linear function where the break point
            locations are known.
        fit_with_breaks_force_points(breaks, x_c, y_c)
            Fit a continuous piecewise linear function where the break point
            locations are known, and force the fit to go through points at x_c
            and y_c.
        predict(x, sorted_data=False, beta=None, breaks=None)
            Evaluate the continuous piecewise linear function at new untested
            points.
        fit_with_breaks_opt(var)
            The objective function to perform a continuous piecewise linear
            fit for a specified number of break points. This is to be used
            with a custom optimization routine, and after use_custom_opt has
            been called.
        fit_force_points_opt(var)&#39;
            Same as fit_with_breaks_opt(var), except this allows for points to
            be forced through x_c and y_c.
        use_custom_opt(n_segments, x_c=None, y_c=None)
            Function to initialize the attributes necessary to use a custom
            optimization routine. Must be used prior to calling
            fit_with_breaks_opt() or fit_force_points_opt().
        calc_slopes()
            Calculate the slopes of the lines after a piecewise linear
            function has been fitted.
        standard_errors()
            Calculate the standard error of each model parameter in the fitted
            piecewise linear function. Note, this assumes no uncertainty in
            break point locations.
        prediction_variance(x, sorted_data=True)
            Calculate the prediction variance at x locations for the fitted
            piecewise linear function. Note, assumes no uncertainty in break
            point locations.
        r_squared()
            Calculate the coefficient of determination, or &#39;R-squared&#39; value
            for a fitted piecewise linear function.

        Examples
        --------
        Initialize for x, y data

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)

        Initialize for x,y data and print optimization results

        &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, disp_res=True)

        If your data is already sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1],
        use sorted_data=True for a slight performance increase while
        initializing the object

        &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, sorted_data=True)
        &#34;&#34;&#34;

        self.print = disp_res

        # x and y should be numpy arrays
        # if they are not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)
        if isinstance(y, np.ndarray) is False:
            y = np.array(y)

        self.sorted_data = sorted_data

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if self.sorted_data:
            self.x_data = x
            self.y_data = y
        else:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            self.x_data = x[order_arg]
            self.y_data = y[order_arg]
        # calculate the number of data points
        self.n_data = len(x)

        # set the first and last break x values to be the min and max of x
        self.break_0 = np.min(self.x_data)
        self.break_n = np.max(self.x_data)

    def fit_with_breaks(self, breaks):
        r&#34;&#34;&#34;
        A function which fits a continuous piecewise linear function
        for specified break point locations.

        The function minimizes the sum of the square of the residuals for the
        x y data.

        If you want to understand the math behind this read
        https://jekel.me/2018/Continous-piecewise-linear-regression/

        Other useful resources:
        http://golovchenko.org/docs/ContinuousPiecewiseLinearFit.pdf
        https://www.mathworks.com/matlabcentral/fileexchange/40913-piecewise-linear-least-square-fit
        http://www.regressionist.com/2018/02/07/continuous-piecewise-linear-fitting/

        Parameters
        ----------
        breaks : array_like
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Returns
        -------
        ssr : float
            Returns the sum of squares of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        The above attributes are added or modified while running this function.

        Examples
        --------
        If your x data exists from 0 &lt;= x &lt;= 1 and you want three
        piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
        and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)

        &#34;&#34;&#34;

        # Check if breaks in ndarray, if not convert to np.array
        if isinstance(breaks, np.ndarray) is False:
            breaks = np.array(breaks)

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        self.fit_breaks = breaks[breaks_order]
        # store the number of parameters and line segments
        self.n_parameters = len(breaks)
        self.n_segments = self.n_parameters - 1

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # try to solve the regression problem
        try:
            # least squares solver
            beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)
            # save the beta parameters
            self.beta = beta

            # save the slopes
            self.calc_slopes()

            # ssr is only calculated if self.n_data &gt; self.n_parameters
            # in this case I&#39;ll need to calculate ssr manually
            # where ssr = sum of square of residuals
            if self.n_data &lt;= self.n_parameters:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

            # if ssr still hasn&#39;t been calculated... Then try again
            if len(ssr) == 0:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return ssr = np.print_function
            # You might have a singular Matrix!!!
            ssr = np.inf
        if ssr is None:
            ssr = np.inf
            # something went wrong...
        return ssr[0]

    def fit_with_breaks_force_points(self, breaks, x_c, y_c):
        r&#34;&#34;&#34;
        A function which fits a continuous piecewise linear function
        for specified break point locations, where you force the
        fit to go through the data points at x_c and y_c.

        The function minimizes the sum of the square of the residuals for the
        pair of x, y data points.

        If you want to understand the math behind this read
        https://jekel.me/2018/Force-piecwise-linear-fit-through-data/

        Parameters
        ----------
        breaks : array_like
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.
        x_c : array_like
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : array_like
            The x locations of the data points that the piecewise linear
            function will be forced to go through.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        zeta : ndarray (1-D)
            The model parameters associated with the constraint function.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        c_n : int
            The number of constraint points. This is the same as len(x_c).


        Returns
        -------
        L : float
            Returns the Lagrangian function value. This is the sum of squares
            of the residuals plus the constraint penalty.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        The above attributes are added or modified while running this function.
        Input:

        Examples
        -------
        If your x data exists from 0 &lt;= x &lt;= 1 and you want three
        piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
        and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
        random. Additionally you desired that the piecewise linear function go
        through the point (0.0, 0.0)

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; x_c = [0.0]
        &gt;&gt;&gt; y_c = [0.0]
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; L = my_pwlf.fit_with_breaks_force_points(breaks, x_c, y_c)

        &#34;&#34;&#34;

        # check if x_c and y_c are numpy array, if not convert to numpy array
        if isinstance(x_c, np.ndarray) is False:
            x_c = np.array(x_c)
        if isinstance(y_c, np.ndarray) is False:
            y_c = np.array(y_c)
        # sort the x_c and y_c data points, then store them
        x_c_order = np.argsort(x_c)
        self.x_c = x_c[x_c_order]
        self.y_c = y_c[x_c_order]
        # store the number of constraints
        self.c_n = len(self.x_c)

        # Check if breaks in ndarray, if not convert to np.array
        if isinstance(breaks, np.ndarray) is False:
            breaks = np.array(breaks)

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        self.fit_breaks = breaks[breaks_order]
        # store the number of parameters and line segments
        self.n_parameters = len(breaks)
        self.n_segments = self.n_parameters - 1

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # Assemble the constraint matrix
        C = np.zeros((self.c_n, self.n_parameters))
        C[:, 0] = 1.0
        C[:, 1] = self.x_c - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = self.x_c &gt; breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

        # Assemble the square constrained least squares matrix
        K = np.zeros((self.n_parameters + self.c_n,
                      self.n_parameters + self.c_n))
        K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
        K[:self.n_parameters, self.n_parameters:] = C.T
        K[self.n_parameters:, :self.n_parameters] = C
        # Assemble right hand side vector
        yt = np.dot(2.0*A.T, self.y_data)
        z = np.zeros(self.n_parameters + self.c_n)
        z[:self.n_parameters] = yt
        z[self.n_parameters:] = self.y_c

        # try to solve the regression problem
        try:
            # Solve the least squares problem
            beta_prime = np.linalg.solve(K, z)

            # save the beta parameters
            self.beta = beta_prime[0:self.n_parameters]
            # save the zeta parameters
            self.zeta = beta_prime[self.n_parameters:]

            # save the slopes
            self.calc_slopes()

            # Calculate ssr
            # where ssr = sum of square of residuals
            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data
            ssr = np.dot(e, e)

            # Calculate the Lagrangian function
            # c_x_y = np.dot(C, self.x_c.T) - self.y_c
            p = np.dot(C.T, self.zeta)
            L = np.sum(np.abs(p)) + ssr

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return L = np.inf
            # You might have a singular Matrix!!!
            L = np.inf
        if L is None:
            L = np.inf
            # something went wrong...
        return L

    def predict(self, x, sorted_data=False, beta=None, breaks=None):
        r&#34;&#34;&#34;
        Evaluate the fitted continuous piecewise linear function at untested
        points.

        You can manfully specify the break points and calculated
        values for beta if you want to quickly predict from different models
        and the same data set.

        Parameters
        ----------
        x : array_like
            The x locations where you want to predict the output of the fitted
            continuous piecewise linear function.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implentation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A processes that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.
        beta : none or ndarray (1-D), optional
            The model parameters for the continuous piecewise linear fit.
            Default is None.
        breaks : none or array_like, optional
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array. Default is None.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.

        Returns
        -------
        y_hat : ndarray (1-D)
            Returns the Lagrangian function value. This is the sum of squares
            of the residuals plus the constraint penalty.

        Notes
        -----
        The above attributes are added or modified if any optional parameter
        is specified.

        Examples
        -------
        Fits a simple model, then predict at x_new locations which are
        linearly spaced.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; yhat = my_pwlf.predict(x_new)

        If the x data is already sorted you can add the sorted_data=True to
        avoid sorting already sorted data.

        &gt;&gt;&gt; yhat = my_pwlf.predict(x_new, sorted_data=False)

        &#34;&#34;&#34;
        if beta is not None and breaks is not None:
            self.beta = beta
            # Sort the breaks, then store them
            breaks_order = np.argsort(breaks)
            self.fit_breaks = breaks[breaks_order]
            self.n_parameters = len(self.fit_breaks)
            self.n_segments = self.n_parameters - 1

        # check if x is numpy array, if not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if sorted_data is False:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            x = x[order_arg]

        # initialize the regression matrix as zeros
        A = np.zeros((len(x), self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = x - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = x &gt; self.fit_breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

        # solve the regression problem
        y_hat = np.dot(A, self.beta)
        return y_hat

    def fit_with_breaks_opt(self, var):
        r&#34;&#34;&#34;
        The objective function to perform a continuous piecewise linear
        fit for a specified number of break points. This is to be used
        with a custom optimization routine, and after use_custom_opt has
        been called.

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        var : array_like
            The break point locations, or variable, in a custom
            optimization routine.

        Returns
        -------
        ssr : float
            The sum of square of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        You should run use_custom_opt to initialize necessary object
        attributes first.

        Unlike fit_with_breaks, fit_with_breaks_opt automatically
        assumes that the first and last break points occur at the min and max
        values of x.
        &#34;&#34;&#34;

        var = np.sort(var)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        breaks = breaks[breaks_order]

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

        # try to solve the regression problem
        try:
            # least squares solver
            beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)

            # ssr is only calculated if self.n_data &gt; self.n_parameters
            # in all other cases I&#39;ll need to calculate ssr manually
            # where ssr = sum of square of residuals
            if self.n_data &lt;= self.n_parameters:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

            # if ssr still hasn&#39;t been calculated... Then try again
            if len(ssr) == 0:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return ssr = np.inf
            # You might have a singular Matrix!!!
            ssr = np.inf
        if ssr is None:
            ssr = np.inf
            # something went wrong...
        return ssr[0]

    def fit_force_points_opt(self, var):
        r&#34;&#34;&#34;
        The objective function to perform a continuous piecewise linear
        fit for a specified number of break points. This is to be used
        with a custom optimization routine, and after use_custom_opt has
        been called.

        Use this function if you intend to be force the model through
        x_c and y_c, while performing a custom optimization.

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        var : array_like
            The break point locations, or variable, in a custom
            optimization routine.

        Returns
        -------
        ssr : float
            The sum of square of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        You should run use_custom_opt to initialize necessary object
        attributes first.

        Unlike fit_with_breaks_force_points, fit_force_points_opt
        automatically assumes that the first and last break points occur
        at the min and max values of x.
        &#34;&#34;&#34;

        var = np.sort(var)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        breaks = breaks[breaks_order]

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

        # Assemble the constraint matrix
        C = np.zeros((self.c_n, self.n_parameters))
        C[:, 0] = 1.0
        C[:, 1] = self.x_c - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = self.x_c &gt; breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

        # Assemble the square constrained least squares matrix
        K = np.zeros((self.n_parameters + self.c_n,
                      self.n_parameters + self.c_n))
        K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
        K[:self.n_parameters, self.n_parameters:] = C.T
        K[self.n_parameters:, :self.n_parameters] = C
        # Assemble right hand side vector
        yt = np.dot(2.0*A.T, self.y_data)
        z = np.zeros(self.n_parameters + self.c_n)
        z[:self.n_parameters] = yt
        z[self.n_parameters:] = self.y_c

        # try to solve the regression problem
        try:
            # Solve the least squares problem
            beta_prime = np.linalg.solve(K, z)

            # save the beta parameters
            self.beta = beta_prime[0:self.n_parameters]
            # save the zeta parameters
            self.zeta = beta_prime[self.n_parameters:]

            # Calculate ssr
            # where ssr = sum of square of residuals
            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data
            ssr = np.dot(e, e)

            # Calculate the Lagrangian function
            p = np.dot(C.T, self.zeta)
            L = np.sum(np.abs(p)) + ssr

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return L = np.inf
            # You might have a singular Matrix!!!
            L = np.inf
        if L is None:
            L = np.inf
            # something went wrong...
        return L

    def fit(self, n_segments, x_c=None, y_c=None, **kwargs):
        r&#34;&#34;&#34;
        Fit a continuous piecewise linear function for a specified number
        of line segments. Uses differential evolution to finds the optimum
        location of break points for a given number of line segments by
        minimizing the sum of the square error.

        Parameters
        ----------
        n_segments : int
            The desired number of line segments.
        x_c : array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        **kwargs : optional
            Directly passed into scipy.optimize.differential_evolution(). This
            will override any pwlf defaults when provided. See Note for more
            information.

        Attributes
        ----------
        ssr : float
            Optimal sum of square error.
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        nVar : int
            The number of variables in the global optimization problem.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        zeta : ndarray (1-D)
            The model parameters associated with the constraint function,
            if x_c and y_c is provided. Only created if x_c and y_c provided.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through. Only created if x_c
            and y_c provided.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through. Only created if x_c
            and y_c provided.
        c_n : int
            The number of constraint points. This is the same as len(x_c).
            Only created if x_c and y_c provided.

        Returns
        -------
        fit_breaks : float
            Break point locations stored as a 1-D numpy array.

        Raises
        ------
        ValueError
            You probably provided x_c without y_c (or vice versa).
            You must provide both x_c and y_c if you plan to force
            the model through data point(s).

        Notes
        -----
        All **kwargs are passed into sicpy.optimize.differential_evolution.
        If any **kwargs is used, it will override my differential_evolution,
        defaults. This allows advanced users to tweak their own optimization.
        For me information see:
        https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

        Examples
        --------
        This example shows you how to fit three continuous piecewise lines to
        a dataset. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fit(3)

        Additionally you desired that the piecewise linear function go
        through the point (0.0, 0.0).

        &gt;&gt;&gt; x_c = [0.0]
        &gt;&gt;&gt; y_c = [0.0]
        &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

        Additionally you desired that the piecewise linear function go
        through the points (0.0, 0.0) and (1.0, 1.0).

        &gt;&gt;&gt; x_c = [0.0, 1.0]
        &gt;&gt;&gt; y_c = [0.0, 1.0]
        &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

        &#34;&#34;&#34;

        # check to see if you&#39;ve provided just x_c or y_c
        logic1 = x_c is not None and y_c is None
        logic2 = y_c is not None and x_c is None
        if logic1 or logic2:
            raise ValueError(&#39;You must provide both x_c and y_c!&#39;)

        # set the function to minimize
        min_function = self.fit_with_breaks_opt

        # if you&#39;ve provided both x_c and y_c
        if x_c is not None and y_c is not None:
            # check if x_c and y_c are numpy array
            # if not convert to numpy array
            if isinstance(x_c, np.ndarray) is False:
                x_c = np.array(x_c)
            if isinstance(y_c, np.ndarray) is False:
                y_c = np.array(y_c)
            # sort the x_c and y_c data points, then store them
            x_c_order = np.argsort(x_c)
            self.x_c = x_c[x_c_order]
            self.y_c = y_c[x_c_order]
            # store the number of constraints
            self.c_n = len(self.x_c)
            # Use a different function to minimize
            min_function = self.fit_force_points_opt

        # store the number of line segments and number of parameters
        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1

        # initiate the bounds of the optimization
        bounds = np.zeros([self.nVar, 2])
        bounds[:, 0] = self.break_0
        bounds[:, 1] = self.break_n

        # run the optimization
        if len(kwargs) == 0:
            res = differential_evolution(min_function, bounds,
                                         strategy=&#39;best1bin&#39;, maxiter=1000,
                                         popsize=50, tol=1e-3,
                                         mutation=(0.5, 1), recombination=0.7,
                                         seed=None, callback=None, disp=False,
                                         polish=True, init=&#39;latinhypercube&#39;,
                                         atol=1e-4)
        else:
            res = differential_evolution(min_function,
                                         bounds, **kwargs)
        if self.print is True:
            print(res)

        self.ssr = res.fun

        # pull the breaks out of the result
        var = np.sort(res.x)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # assign values
        if x_c is None and y_c is None:
            self.fit_with_breaks(breaks)
        else:
            self.fit_with_breaks_force_points(breaks, self.x_c, self.y_c)

        return self.fit_breaks

    def fitfast(self, n_segments, pop=2, **kwargs):
        r&#34;&#34;&#34;
        Uses multi start LBFGSB optimization to find the location of
        break points for a given number of line segments by minimizing the sum
        of the square of the errors.

        The idea is that we generate n random latin hypercube samples
        and run LBFGSB optimization on each one. This isn&#39;t guaranteed to
        find the global optimum. It&#39;s suppose to be a reasonable compromise
        between speed and quality of fit. Let me know how it works.

        Since this is based on random sampling, you might want to run it
        multiple times and save the best version... The best version will
        have the lowest self.ssr (sum of square of residuals).

        There is no guarantee that this will be faster than fit(), however
        you may find it much faster sometimes.

        Parameters
        ----------
        n_segments : int
            The desired number of line segments.
        pop : int, optional
            The number of latin hypercube samples to generate. Default pop=2.
        **kwargs : optional
            Directly passed into scipy.optimize.differential_evolution(). This
            will override any pwlf defaults when provided. See Note for more
            information.

        Attributes
        ----------
        ssr : float
            Optimal sum of square error.
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        nVar : int
            The number of variables in the global optimization problem.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Returns
        -------
        fit_breaks : float
            Break point locations stored as a 1-D numpy array.

        Notes
        -----
        The default number of multi start optimizations is 2.
            - Decreasing this number will result in a faster run time.
            - Increasing this number will improve the likelihood of finding
                good results
            - You can specify the number of starts using the following call
            - Minimum value of pop is 2

        All **kwargs are passed into sicpy.optimize.fmin_l_bfgs_b. If any
        **kwargs is used, it will override my defaults. This allows
        advanced users to tweak their own optimization. For me information see:
        https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

        Examples
        --------
        This example shows you how to fit three continuous piecewise lines to
        a dataset. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)

        You can change the number of latin hypercube samples (or starting
        point, locations) to use with pop. The following example will use 50
        samples.

        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3, pop=50)

        &#34;&#34;&#34;
        pop = int(pop)  # ensure that the population is integer

        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1

        # initiate the bounds of the optimization
        bounds = np.zeros([self.nVar, 2])
        bounds[:, 0] = self.break_0
        bounds[:, 1] = self.break_n

        # perform latin hypercube sampling
        mypop = lhs(self.nVar, samples=pop, criterion=&#39;maximin&#39;)
        # scale the sampling to my variable range
        mypop = mypop * (self.break_n - self.break_0) + self.break_0

        x = np.zeros((pop, self.nVar))
        f = np.zeros(pop)
        d = []

        for i, x0 in enumerate(mypop):
            if len(kwargs) == 0:
                resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                                 fprime=None, args=(),
                                                 approx_grad=True,
                                                 bounds=bounds, m=10,
                                                 factr=1e2, pgtol=1e-05,
                                                 epsilon=1e-08, iprint=-1,
                                                 maxfun=15000, maxiter=15000,
                                                 disp=None, callback=None)
            else:
                resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                                 fprime=None, approx_grad=True,
                                                 bounds=bounds, **kwargs)
            x[i, :] = resx
            f[i] = resf
            d.append(resd)
            if self.print is True:
                print(i + 1, &#39;of &#39; + str(pop) + &#39; complete&#39;)

        # find the best result
        best_ind = np.nanargmin(f)
        best_val = f[best_ind]
        best_break = x[best_ind]
        res = (x[best_ind], f[best_ind], d[best_ind])
        if self.print is True:
            print(res)

        self.ssr = best_val

        # obtain the break point locations from the best result
        var = np.sort(best_break)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # assign parameters
        self.fit_with_breaks(breaks)

        return self.fit_breaks

    def use_custom_opt(self, n_segments, x_c=None, y_c=None):
        r&#34;&#34;&#34;
        Provide the number of line segments you want to use with your
        custom optimization routine.

        Run this function first to initialize necessary attributes!!!

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        n_segments : int
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.
        x_c : none or array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : none or array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.

        Attributes
        ----------
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        nVar : int
            The number of variables in the global optimization problem.
        n_segments : int
            The number of line segments.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        c_n : int
            The number of constraint points. This is the same as len(x_c).

        Notes
        -----
        Optimize fit_with_breaks_opt(var) where var is a 1D array
        containing the x locations of your variables
        var has length n_segments - 1, because the two break points
        are always defined (1. the min of x, 2. the max of x).

        fit_with_breaks_opt(var) will return the sum of the square of the
        residuals which you&#39;ll want to minimize with your optimization
        routine.
        &#34;&#34;&#34;

        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1
        if x_c is not None or y_c is not None:
            # check if x_c and y_c are numpy array
            # if not convert to numpy array
            if isinstance(x_c, np.ndarray) is False:
                x_c = np.array(x_c)
            if isinstance(y_c, np.ndarray) is False:
                y_c = np.array(y_c)
            # sort the x_c and y_c data points, then store them
            x_c_order = np.argsort(x_c)
            self.x_c = x_c[x_c_order]
            self.y_c = y_c[x_c_order]
            # store the number of constraints
            self.c_n = len(self.x_c)

    def calc_slopes(self):
        r&#34;&#34;&#34;
        Calculate the slopes of the lines after a piecewise linear
        function has been fitted.

        This will also calculate the y-intercept from each line in the form
        y = mx + b. The intercepts are stored at self.intercepts.

        Attributes
        ----------
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        intercepts : ndarray (1-D)
            The y-intercept of each line segment as a 1-D numpy array.

        Returns
        -------
        slopes : ndarray(1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Examples
        --------
        Calculate the slopes after performing a simple fit

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fit(3)
        &gt;&gt;&gt; slopes = my_pwlf.slopes()

        &#34;&#34;&#34;
        y_hat = self.predict(self.fit_breaks)
        self.slopes = np.zeros(self.n_segments)
        for i in range(self.n_segments):
            self.slopes[i] = (y_hat[i+1]-y_hat[i]) / \
                        (self.fit_breaks[i+1]-self.fit_breaks[i])
        self.intercepts = y_hat[0:-1] - self.slopes*self.fit_breaks[0:-1]
        return self.slopes

    def standard_errors(self):
        r&#34;&#34;&#34;
        Calculate the standard errors for each beta parameter determined
        from the piecewise linear fit. Typically +- 1.96*se will yield the
        center of a 95% confidence region around your parameters. This
        assumes the parmaters follow a normal distribution. For more
        information see:
        https://en.wikipedia.org/wiki/Standard_error

        This calculation follows the derivation provided in [1]_. A taylor-
        series expansion is not needed since this is linear regression.

        Returns
        -------
        se : ndarray (1-D)
            Standard errors associated with each beta parameter. Specifically
            se[0] correspounds to the standard error for beta[0], and so forth.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        Note, this assumes no uncertainty in break point locations.

        References
        ----------
        .. [1] Coppe, A., Haftka, R. T., and Kim, N. H., “Uncertainty
            Identification of Damage Growth Parameters Using Nonlinear
            Regression,” AIAA Journal, Vol. 49, No. 12, dec 2011, pp.
            2818–2821.

        Examples
        --------
        Calculate the standard errors after performing a simple fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; se = my_pwlf.standard_errors()

        &#34;&#34;&#34;
        try:
            nb = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)

        ny = len(self.y_data)

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # try to solve for the standard errors
        try:

            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data

            # solve for the unbiased estimate of variance
            variance = np.dot(e, e) / (ny - nb)

            self.se = np.sqrt(variance * (np.linalg.inv(np.dot(A.T,
                                                               A)).diagonal()))

            return self.se

        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def prediction_variance(self, x, sorted_data=False):
        r&#34;&#34;&#34;
        Calculate the prediction variance for each specified x location. The
        prediction variance is the uncertainty of the model due to the lack of
        data. This can be used to find a 95% confidence interval of possible
        piecewise linear models based on the current data. This would be
        done typically as y_hat +- 1.96*np.sqrt(pre_var). The
        prediction_variance needs to be calculated at various x locations.
        For more information see:
        www2.mae.ufl.edu/haftka/vvuq/lectures/Regression-accuracy.pptx

        Parameters
        ----------
        x : array_like
            The x locations where you want the prediction variance from the
            fitted continuous piecewise linear function.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implentation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A processes that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.

        Returns
        -------
        pre_var : ndarray (1-D)
            Numpy array (floats) of prediction variance at each x location.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        This assumes that your break point locations are exact! and does
        not consider the uncertainty with your break point locations.

        Examples
        --------
        Calculate the prediction variance at x_new after performing a simple
        fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; pre_var = my_pwlf.prediction_variance(x_new)

        see also examples/prediction_variance.py

        &#34;&#34;&#34;
        try:
            nb = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)

        ny = len(self.y_data)

        # check if x is numpy array, if not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if sorted_data is False:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            x = x[order_arg]

        # calculate the prediction variance
        Ad = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        Ad[:, 0] = 1.0
        Ad[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            Ad[int_index:, i+2] = self.x_data[int_index:] - \
                self.fit_breaks[i+1]

        # try to solve for the unbiased variance estimation
        try:

            y_hat = np.dot(Ad, self.beta)
            e = y_hat - self.y_data

            # solve for the unbiased estimate of variance
            variance = np.dot(e, e) / (ny - nb)

        except np.linalg.LinAlgError:
            raise(&#34;Unable to calculate prediction variance.&#34;
                  &#34; Something went wrong.&#34;)

        # initialize the regression matrix as zeros
        A = np.zeros((len(x), self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = x - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = x &gt; self.fit_breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

        # try to solve for the prediction variance at the x locations
        try:
            pre_var = variance * \
                np.dot(np.dot(A, np.linalg.inv(np.dot(Ad.T, Ad))), A.T)
            return pre_var.diagonal()

        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def r_squared(self):
        r&#34;&#34;&#34;
        Calculate the coefficient of determination (&#34;R squared&#34;, R^2) value
        after a fit has been performed.
        For more information see:
        https://en.wikipedia.org/wiki/Coefficient_of_determination

        Returns
        -------
        rsq : float
            Coefficient of determination, or &#39;R squared&#39; value.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Examples
        --------
        Calculate the R squared value after performing a simple fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; rsq = PiecewiseLinFit.r_squared()

        &#34;&#34;&#34;
        try:
            fit_breaks = self.fit_breaks
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)
        ssr = self.fit_with_breaks(fit_breaks)
        ybar = np.ones(self.n_data) * np.mean(self.y_data)
        ydiff = self.y_data - ybar
        try:
            sst = np.dot(ydiff, ydiff)
            rsq = 1.0 - (ssr/sst)
            return rsq
        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def p_values(self):
        r&#34;&#34;&#34;
        Calculate the p-values for each beta parameter.

        This calculates the p-values for the beta parameters under the
        assumption that your break point locations are known. Section 2.4.2 of
        [1]_ defines how to calculate the p-value of individual parameters.
        This is really a marginal test since each parameter is dependent upon
        the other parameters.

        Returns
        -------
        p : ndarray (1-D)
            p-values for each beta parameter where p-value[0] corresponds to
            beta[0] and so forth

        Raises
        ValueError
            You have probably not performed a fit yet.

        Notes
        -----
        This assumes that your break point locations are exact! and does
        not consider the uncertainty with your break point locations.

        See https://github.com/cjekel/piecewise_linear_fit_py/issues/14

        References
        ----------
        .. [1] Myers RH, Montgomery DC, Anderson-Cook CM. Response surface
            methodology . Hoboken. New Jersey: John Wiley &amp; Sons, Inc.
            2009;20:38-44.

        Examples
        --------
        After performing a fit, one can calculate the p-value for each beta
        parameter

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; p = my_pwlf.p_values(x_new)

        see also examples/standard_errrors_and_p-values.py

        &#34;&#34;&#34;
        # calculate the standard errors associated with each beta parameter
        # not that these standard errors and p-values are only meaningful if
        # you have specified the specific line segment end locations
        # at least for now...
        self.standard_errors()

        # calculate my t-value
        t = self.beta / self.se

        # degrees of freedom for t-distribution
        n = self.n_data
        try:
            k = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)
        # calculate the p-values
        p = stats.t.sf(np.abs(t), df=n-k-1)
        return p}</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pwlf.pwlf.PiecewiseLinFit"><code class="flex name class">
<span>class <span class="ident">PiecewiseLinFit</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class PiecewiseLinFit(object):

    def __init__(self, x, y, disp_res=False, sorted_data=False):
        r&#34;&#34;&#34;
        An object to fit a continuous piecewise linear function
        to data.

        Initiate the library with the supplied x and y data.
        Supply the x and y data of which you&#39;ll be fitting
        a continuous piecewise linear model to where y(x).
        by default pwlf won&#39;t print the optimization results.;

        Parameters
        ----------
        x : array_like
            The x or independent data point locations as list or 1 dimensional
            numpy array. The x and y data should be ordered such that x[i]
            corresponds to y[i], for an arbitrary index i.
        y : array_like
            The y or dependent data point locations as list or 1 dimensional
            numpy array.
        disp_res : bool, optional
            Whether the optimization results should be printed. Default is
            False.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implementation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A process that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.

        Attributes
        --------
        x_data : ndarray (1-D)
            The inputted parameter x from the 1-D data set.
        y_data : ndarray (1-D)
            The inputted parameter y from the 1-D data set.
        n_data : int
            The number of data points.
        break_0 : float
            The smallest x value.
        break_n : float
            The largest x value.
        print : bool
            Whether the optimization results should be printed. Default is
            False.

        Methods
        -------
        fit(n_segments, x_c=None, y_c=None, **kwargs)
            Fit a continuous piecewise linear function for a specified number
            of line segments.
        fitfast(n_segments, pop=2, **kwargs)
            Fit a continuous piecewise linear function for a specified number
            of line segments using a specialized optimization routine that
            should be faster than fit() for large problems. The tradeoff may
            be that fitfast() results in a lower quality model.
        fit_with_breaks(breaks)
            Fit a continuous piecewise linear function where the break point
            locations are known.
        fit_with_breaks_force_points(breaks, x_c, y_c)
            Fit a continuous piecewise linear function where the break point
            locations are known, and force the fit to go through points at x_c
            and y_c.
        predict(x, sorted_data=False, beta=None, breaks=None)
            Evaluate the continuous piecewise linear function at new untested
            points.
        fit_with_breaks_opt(var)
            The objective function to perform a continuous piecewise linear
            fit for a specified number of break points. This is to be used
            with a custom optimization routine, and after use_custom_opt has
            been called.
        fit_force_points_opt(var)&#39;
            Same as fit_with_breaks_opt(var), except this allows for points to
            be forced through x_c and y_c.
        use_custom_opt(n_segments, x_c=None, y_c=None)
            Function to initialize the attributes necessary to use a custom
            optimization routine. Must be used prior to calling
            fit_with_breaks_opt() or fit_force_points_opt().
        calc_slopes()
            Calculate the slopes of the lines after a piecewise linear
            function has been fitted.
        standard_errors()
            Calculate the standard error of each model parameter in the fitted
            piecewise linear function. Note, this assumes no uncertainty in
            break point locations.
        prediction_variance(x, sorted_data=True)
            Calculate the prediction variance at x locations for the fitted
            piecewise linear function. Note, assumes no uncertainty in break
            point locations.
        r_squared()
            Calculate the coefficient of determination, or &#39;R-squared&#39; value
            for a fitted piecewise linear function.

        Examples
        --------
        Initialize for x, y data

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)

        Initialize for x,y data and print optimization results

        &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, disp_res=True)

        If your data is already sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1],
        use sorted_data=True for a slight performance increase while
        initializing the object

        &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, sorted_data=True)
        &#34;&#34;&#34;

        self.print = disp_res

        # x and y should be numpy arrays
        # if they are not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)
        if isinstance(y, np.ndarray) is False:
            y = np.array(y)

        self.sorted_data = sorted_data

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if self.sorted_data:
            self.x_data = x
            self.y_data = y
        else:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            self.x_data = x[order_arg]
            self.y_data = y[order_arg]
        # calculate the number of data points
        self.n_data = len(x)

        # set the first and last break x values to be the min and max of x
        self.break_0 = np.min(self.x_data)
        self.break_n = np.max(self.x_data)

    def fit_with_breaks(self, breaks):
        r&#34;&#34;&#34;
        A function which fits a continuous piecewise linear function
        for specified break point locations.

        The function minimizes the sum of the square of the residuals for the
        x y data.

        If you want to understand the math behind this read
        https://jekel.me/2018/Continous-piecewise-linear-regression/

        Other useful resources:
        http://golovchenko.org/docs/ContinuousPiecewiseLinearFit.pdf
        https://www.mathworks.com/matlabcentral/fileexchange/40913-piecewise-linear-least-square-fit
        http://www.regressionist.com/2018/02/07/continuous-piecewise-linear-fitting/

        Parameters
        ----------
        breaks : array_like
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Returns
        -------
        ssr : float
            Returns the sum of squares of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        The above attributes are added or modified while running this function.

        Examples
        --------
        If your x data exists from 0 &lt;= x &lt;= 1 and you want three
        piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
        and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)

        &#34;&#34;&#34;

        # Check if breaks in ndarray, if not convert to np.array
        if isinstance(breaks, np.ndarray) is False:
            breaks = np.array(breaks)

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        self.fit_breaks = breaks[breaks_order]
        # store the number of parameters and line segments
        self.n_parameters = len(breaks)
        self.n_segments = self.n_parameters - 1

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # try to solve the regression problem
        try:
            # least squares solver
            beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)
            # save the beta parameters
            self.beta = beta

            # save the slopes
            self.calc_slopes()

            # ssr is only calculated if self.n_data &gt; self.n_parameters
            # in this case I&#39;ll need to calculate ssr manually
            # where ssr = sum of square of residuals
            if self.n_data &lt;= self.n_parameters:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

            # if ssr still hasn&#39;t been calculated... Then try again
            if len(ssr) == 0:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return ssr = np.print_function
            # You might have a singular Matrix!!!
            ssr = np.inf
        if ssr is None:
            ssr = np.inf
            # something went wrong...
        return ssr[0]

    def fit_with_breaks_force_points(self, breaks, x_c, y_c):
        r&#34;&#34;&#34;
        A function which fits a continuous piecewise linear function
        for specified break point locations, where you force the
        fit to go through the data points at x_c and y_c.

        The function minimizes the sum of the square of the residuals for the
        pair of x, y data points.

        If you want to understand the math behind this read
        https://jekel.me/2018/Force-piecwise-linear-fit-through-data/

        Parameters
        ----------
        breaks : array_like
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.
        x_c : array_like
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : array_like
            The x locations of the data points that the piecewise linear
            function will be forced to go through.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        zeta : ndarray (1-D)
            The model parameters associated with the constraint function.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        c_n : int
            The number of constraint points. This is the same as len(x_c).


        Returns
        -------
        L : float
            Returns the Lagrangian function value. This is the sum of squares
            of the residuals plus the constraint penalty.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        The above attributes are added or modified while running this function.
        Input:

        Examples
        -------
        If your x data exists from 0 &lt;= x &lt;= 1 and you want three
        piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
        and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
        random. Additionally you desired that the piecewise linear function go
        through the point (0.0, 0.0)

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; x_c = [0.0]
        &gt;&gt;&gt; y_c = [0.0]
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; L = my_pwlf.fit_with_breaks_force_points(breaks, x_c, y_c)

        &#34;&#34;&#34;

        # check if x_c and y_c are numpy array, if not convert to numpy array
        if isinstance(x_c, np.ndarray) is False:
            x_c = np.array(x_c)
        if isinstance(y_c, np.ndarray) is False:
            y_c = np.array(y_c)
        # sort the x_c and y_c data points, then store them
        x_c_order = np.argsort(x_c)
        self.x_c = x_c[x_c_order]
        self.y_c = y_c[x_c_order]
        # store the number of constraints
        self.c_n = len(self.x_c)

        # Check if breaks in ndarray, if not convert to np.array
        if isinstance(breaks, np.ndarray) is False:
            breaks = np.array(breaks)

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        self.fit_breaks = breaks[breaks_order]
        # store the number of parameters and line segments
        self.n_parameters = len(breaks)
        self.n_segments = self.n_parameters - 1

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # Assemble the constraint matrix
        C = np.zeros((self.c_n, self.n_parameters))
        C[:, 0] = 1.0
        C[:, 1] = self.x_c - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = self.x_c &gt; breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

        # Assemble the square constrained least squares matrix
        K = np.zeros((self.n_parameters + self.c_n,
                      self.n_parameters + self.c_n))
        K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
        K[:self.n_parameters, self.n_parameters:] = C.T
        K[self.n_parameters:, :self.n_parameters] = C
        # Assemble right hand side vector
        yt = np.dot(2.0*A.T, self.y_data)
        z = np.zeros(self.n_parameters + self.c_n)
        z[:self.n_parameters] = yt
        z[self.n_parameters:] = self.y_c

        # try to solve the regression problem
        try:
            # Solve the least squares problem
            beta_prime = np.linalg.solve(K, z)

            # save the beta parameters
            self.beta = beta_prime[0:self.n_parameters]
            # save the zeta parameters
            self.zeta = beta_prime[self.n_parameters:]

            # save the slopes
            self.calc_slopes()

            # Calculate ssr
            # where ssr = sum of square of residuals
            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data
            ssr = np.dot(e, e)

            # Calculate the Lagrangian function
            # c_x_y = np.dot(C, self.x_c.T) - self.y_c
            p = np.dot(C.T, self.zeta)
            L = np.sum(np.abs(p)) + ssr

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return L = np.inf
            # You might have a singular Matrix!!!
            L = np.inf
        if L is None:
            L = np.inf
            # something went wrong...
        return L

    def predict(self, x, sorted_data=False, beta=None, breaks=None):
        r&#34;&#34;&#34;
        Evaluate the fitted continuous piecewise linear function at untested
        points.

        You can manfully specify the break points and calculated
        values for beta if you want to quickly predict from different models
        and the same data set.

        Parameters
        ----------
        x : array_like
            The x locations where you want to predict the output of the fitted
            continuous piecewise linear function.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implentation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A processes that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.
        beta : none or ndarray (1-D), optional
            The model parameters for the continuous piecewise linear fit.
            Default is None.
        breaks : none or array_like, optional
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array. Default is None.

        Attributes
        ----------
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.

        Returns
        -------
        y_hat : ndarray (1-D)
            Returns the Lagrangian function value. This is the sum of squares
            of the residuals plus the constraint penalty.

        Notes
        -----
        The above attributes are added or modified if any optional parameter
        is specified.

        Examples
        -------
        Fits a simple model, then predict at x_new locations which are
        linearly spaced.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
        &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; yhat = my_pwlf.predict(x_new)

        If the x data is already sorted you can add the sorted_data=True to
        avoid sorting already sorted data.

        &gt;&gt;&gt; yhat = my_pwlf.predict(x_new, sorted_data=False)

        &#34;&#34;&#34;
        if beta is not None and breaks is not None:
            self.beta = beta
            # Sort the breaks, then store them
            breaks_order = np.argsort(breaks)
            self.fit_breaks = breaks[breaks_order]
            self.n_parameters = len(self.fit_breaks)
            self.n_segments = self.n_parameters - 1

        # check if x is numpy array, if not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if sorted_data is False:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            x = x[order_arg]

        # initialize the regression matrix as zeros
        A = np.zeros((len(x), self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = x - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = x &gt; self.fit_breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

        # solve the regression problem
        y_hat = np.dot(A, self.beta)
        return y_hat

    def fit_with_breaks_opt(self, var):
        r&#34;&#34;&#34;
        The objective function to perform a continuous piecewise linear
        fit for a specified number of break points. This is to be used
        with a custom optimization routine, and after use_custom_opt has
        been called.

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        var : array_like
            The break point locations, or variable, in a custom
            optimization routine.

        Returns
        -------
        ssr : float
            The sum of square of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        You should run use_custom_opt to initialize necessary object
        attributes first.

        Unlike fit_with_breaks, fit_with_breaks_opt automatically
        assumes that the first and last break points occur at the min and max
        values of x.
        &#34;&#34;&#34;

        var = np.sort(var)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        breaks = breaks[breaks_order]

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

        # try to solve the regression problem
        try:
            # least squares solver
            beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)

            # ssr is only calculated if self.n_data &gt; self.n_parameters
            # in all other cases I&#39;ll need to calculate ssr manually
            # where ssr = sum of square of residuals
            if self.n_data &lt;= self.n_parameters:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

            # if ssr still hasn&#39;t been calculated... Then try again
            if len(ssr) == 0:
                y_hat = np.dot(A, beta)
                e = y_hat - self.y_data
                ssr = [np.dot(e, e)]

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return ssr = np.inf
            # You might have a singular Matrix!!!
            ssr = np.inf
        if ssr is None:
            ssr = np.inf
            # something went wrong...
        return ssr[0]

    def fit_force_points_opt(self, var):
        r&#34;&#34;&#34;
        The objective function to perform a continuous piecewise linear
        fit for a specified number of break points. This is to be used
        with a custom optimization routine, and after use_custom_opt has
        been called.

        Use this function if you intend to be force the model through
        x_c and y_c, while performing a custom optimization.

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        var : array_like
            The break point locations, or variable, in a custom
            optimization routine.

        Returns
        -------
        ssr : float
            The sum of square of the residuals.

        Raises
        ------
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        You should run use_custom_opt to initialize necessary object
        attributes first.

        Unlike fit_with_breaks_force_points, fit_force_points_opt
        automatically assumes that the first and last break points occur
        at the min and max values of x.
        &#34;&#34;&#34;

        var = np.sort(var)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        breaks = breaks[breaks_order]

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

        # Assemble the constraint matrix
        C = np.zeros((self.c_n, self.n_parameters))
        C[:, 0] = 1.0
        C[:, 1] = self.x_c - breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = self.x_c &gt; breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

        # Assemble the square constrained least squares matrix
        K = np.zeros((self.n_parameters + self.c_n,
                      self.n_parameters + self.c_n))
        K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
        K[:self.n_parameters, self.n_parameters:] = C.T
        K[self.n_parameters:, :self.n_parameters] = C
        # Assemble right hand side vector
        yt = np.dot(2.0*A.T, self.y_data)
        z = np.zeros(self.n_parameters + self.c_n)
        z[:self.n_parameters] = yt
        z[self.n_parameters:] = self.y_c

        # try to solve the regression problem
        try:
            # Solve the least squares problem
            beta_prime = np.linalg.solve(K, z)

            # save the beta parameters
            self.beta = beta_prime[0:self.n_parameters]
            # save the zeta parameters
            self.zeta = beta_prime[self.n_parameters:]

            # Calculate ssr
            # where ssr = sum of square of residuals
            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data
            ssr = np.dot(e, e)

            # Calculate the Lagrangian function
            p = np.dot(C.T, self.zeta)
            L = np.sum(np.abs(p)) + ssr

        except np.linalg.LinAlgError:
            # the computation could not converge!
            # on an error, return L = np.inf
            # You might have a singular Matrix!!!
            L = np.inf
        if L is None:
            L = np.inf
            # something went wrong...
        return L

    def fit(self, n_segments, x_c=None, y_c=None, **kwargs):
        r&#34;&#34;&#34;
        Fit a continuous piecewise linear function for a specified number
        of line segments. Uses differential evolution to finds the optimum
        location of break points for a given number of line segments by
        minimizing the sum of the square error.

        Parameters
        ----------
        n_segments : int
            The desired number of line segments.
        x_c : array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        **kwargs : optional
            Directly passed into scipy.optimize.differential_evolution(). This
            will override any pwlf defaults when provided. See Note for more
            information.

        Attributes
        ----------
        ssr : float
            Optimal sum of square error.
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        nVar : int
            The number of variables in the global optimization problem.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        zeta : ndarray (1-D)
            The model parameters associated with the constraint function,
            if x_c and y_c is provided. Only created if x_c and y_c provided.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through. Only created if x_c
            and y_c provided.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through. Only created if x_c
            and y_c provided.
        c_n : int
            The number of constraint points. This is the same as len(x_c).
            Only created if x_c and y_c provided.

        Returns
        -------
        fit_breaks : float
            Break point locations stored as a 1-D numpy array.

        Raises
        ------
        ValueError
            You probably provided x_c without y_c (or vice versa).
            You must provide both x_c and y_c if you plan to force
            the model through data point(s).

        Notes
        -----
        All **kwargs are passed into sicpy.optimize.differential_evolution.
        If any **kwargs is used, it will override my differential_evolution,
        defaults. This allows advanced users to tweak their own optimization.
        For me information see:
        https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

        Examples
        --------
        This example shows you how to fit three continuous piecewise lines to
        a dataset. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fit(3)

        Additionally you desired that the piecewise linear function go
        through the point (0.0, 0.0).

        &gt;&gt;&gt; x_c = [0.0]
        &gt;&gt;&gt; y_c = [0.0]
        &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

        Additionally you desired that the piecewise linear function go
        through the points (0.0, 0.0) and (1.0, 1.0).

        &gt;&gt;&gt; x_c = [0.0, 1.0]
        &gt;&gt;&gt; y_c = [0.0, 1.0]
        &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

        &#34;&#34;&#34;

        # check to see if you&#39;ve provided just x_c or y_c
        logic1 = x_c is not None and y_c is None
        logic2 = y_c is not None and x_c is None
        if logic1 or logic2:
            raise ValueError(&#39;You must provide both x_c and y_c!&#39;)

        # set the function to minimize
        min_function = self.fit_with_breaks_opt

        # if you&#39;ve provided both x_c and y_c
        if x_c is not None and y_c is not None:
            # check if x_c and y_c are numpy array
            # if not convert to numpy array
            if isinstance(x_c, np.ndarray) is False:
                x_c = np.array(x_c)
            if isinstance(y_c, np.ndarray) is False:
                y_c = np.array(y_c)
            # sort the x_c and y_c data points, then store them
            x_c_order = np.argsort(x_c)
            self.x_c = x_c[x_c_order]
            self.y_c = y_c[x_c_order]
            # store the number of constraints
            self.c_n = len(self.x_c)
            # Use a different function to minimize
            min_function = self.fit_force_points_opt

        # store the number of line segments and number of parameters
        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1

        # initiate the bounds of the optimization
        bounds = np.zeros([self.nVar, 2])
        bounds[:, 0] = self.break_0
        bounds[:, 1] = self.break_n

        # run the optimization
        if len(kwargs) == 0:
            res = differential_evolution(min_function, bounds,
                                         strategy=&#39;best1bin&#39;, maxiter=1000,
                                         popsize=50, tol=1e-3,
                                         mutation=(0.5, 1), recombination=0.7,
                                         seed=None, callback=None, disp=False,
                                         polish=True, init=&#39;latinhypercube&#39;,
                                         atol=1e-4)
        else:
            res = differential_evolution(min_function,
                                         bounds, **kwargs)
        if self.print is True:
            print(res)

        self.ssr = res.fun

        # pull the breaks out of the result
        var = np.sort(res.x)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # assign values
        if x_c is None and y_c is None:
            self.fit_with_breaks(breaks)
        else:
            self.fit_with_breaks_force_points(breaks, self.x_c, self.y_c)

        return self.fit_breaks

    def fitfast(self, n_segments, pop=2, **kwargs):
        r&#34;&#34;&#34;
        Uses multi start LBFGSB optimization to find the location of
        break points for a given number of line segments by minimizing the sum
        of the square of the errors.

        The idea is that we generate n random latin hypercube samples
        and run LBFGSB optimization on each one. This isn&#39;t guaranteed to
        find the global optimum. It&#39;s suppose to be a reasonable compromise
        between speed and quality of fit. Let me know how it works.

        Since this is based on random sampling, you might want to run it
        multiple times and save the best version... The best version will
        have the lowest self.ssr (sum of square of residuals).

        There is no guarantee that this will be faster than fit(), however
        you may find it much faster sometimes.

        Parameters
        ----------
        n_segments : int
            The desired number of line segments.
        pop : int, optional
            The number of latin hypercube samples to generate. Default pop=2.
        **kwargs : optional
            Directly passed into scipy.optimize.differential_evolution(). This
            will override any pwlf defaults when provided. See Note for more
            information.

        Attributes
        ----------
        ssr : float
            Optimal sum of square error.
        fit_breaks : ndarray (1-D)
            Break point locations stored as a 1-D numpy array.
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        n_segments : int
            The number of line segments.
        nVar : int
            The number of variables in the global optimization problem.
        beta : ndarray (1-D)
            The model parameters for the continuous piecewise linear fit.
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Returns
        -------
        fit_breaks : float
            Break point locations stored as a 1-D numpy array.

        Notes
        -----
        The default number of multi start optimizations is 2.
            - Decreasing this number will result in a faster run time.
            - Increasing this number will improve the likelihood of finding
                good results
            - You can specify the number of starts using the following call
            - Minimum value of pop is 2

        All **kwargs are passed into sicpy.optimize.fmin_l_bfgs_b. If any
        **kwargs is used, it will override my defaults. This allows
        advanced users to tweak their own optimization. For me information see:
        https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

        Examples
        --------
        This example shows you how to fit three continuous piecewise lines to
        a dataset. This assumes that x is linearly spaced from [0, 1), and y is
        random.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)

        You can change the number of latin hypercube samples (or starting
        point, locations) to use with pop. The following example will use 50
        samples.

        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3, pop=50)

        &#34;&#34;&#34;
        pop = int(pop)  # ensure that the population is integer

        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1

        # initiate the bounds of the optimization
        bounds = np.zeros([self.nVar, 2])
        bounds[:, 0] = self.break_0
        bounds[:, 1] = self.break_n

        # perform latin hypercube sampling
        mypop = lhs(self.nVar, samples=pop, criterion=&#39;maximin&#39;)
        # scale the sampling to my variable range
        mypop = mypop * (self.break_n - self.break_0) + self.break_0

        x = np.zeros((pop, self.nVar))
        f = np.zeros(pop)
        d = []

        for i, x0 in enumerate(mypop):
            if len(kwargs) == 0:
                resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                                 fprime=None, args=(),
                                                 approx_grad=True,
                                                 bounds=bounds, m=10,
                                                 factr=1e2, pgtol=1e-05,
                                                 epsilon=1e-08, iprint=-1,
                                                 maxfun=15000, maxiter=15000,
                                                 disp=None, callback=None)
            else:
                resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                                 fprime=None, approx_grad=True,
                                                 bounds=bounds, **kwargs)
            x[i, :] = resx
            f[i] = resf
            d.append(resd)
            if self.print is True:
                print(i + 1, &#39;of &#39; + str(pop) + &#39; complete&#39;)

        # find the best result
        best_ind = np.nanargmin(f)
        best_val = f[best_ind]
        best_break = x[best_ind]
        res = (x[best_ind], f[best_ind], d[best_ind])
        if self.print is True:
            print(res)

        self.ssr = best_val

        # obtain the break point locations from the best result
        var = np.sort(best_break)
        breaks = np.zeros(len(var) + 2)
        breaks[1:-1] = var.copy()
        breaks[0] = self.break_0
        breaks[-1] = self.break_n

        # assign parameters
        self.fit_with_breaks(breaks)

        return self.fit_breaks

    def use_custom_opt(self, n_segments, x_c=None, y_c=None):
        r&#34;&#34;&#34;
        Provide the number of line segments you want to use with your
        custom optimization routine.

        Run this function first to initialize necessary attributes!!!

        This was intended for advanced users only.

        See the following example
        https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

        Parameters
        ----------
        n_segments : int
            The x locations where each line segment terminates. These are
            referred to as break points for each line segment. This should be
            structured as a 1-D numpy array.
        x_c : none or array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : none or array_like, optional
            The x locations of the data points that the piecewise linear
            function will be forced to go through.

        Attributes
        ----------
        n_parameters : int
            The number of model parameters. This is equivalent to the
            len(beta).
        nVar : int
            The number of variables in the global optimization problem.
        n_segments : int
            The number of line segments.
        x_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        y_c : ndarray (1-D)
            The x locations of the data points that the piecewise linear
            function will be forced to go through.
        c_n : int
            The number of constraint points. This is the same as len(x_c).

        Notes
        -----
        Optimize fit_with_breaks_opt(var) where var is a 1D array
        containing the x locations of your variables
        var has length n_segments - 1, because the two break points
        are always defined (1. the min of x, 2. the max of x).

        fit_with_breaks_opt(var) will return the sum of the square of the
        residuals which you&#39;ll want to minimize with your optimization
        routine.
        &#34;&#34;&#34;

        self.n_segments = int(n_segments)
        self.n_parameters = self.n_segments + 1

        # calculate the number of variables I have to solve for
        self.nVar = self.n_segments - 1
        if x_c is not None or y_c is not None:
            # check if x_c and y_c are numpy array
            # if not convert to numpy array
            if isinstance(x_c, np.ndarray) is False:
                x_c = np.array(x_c)
            if isinstance(y_c, np.ndarray) is False:
                y_c = np.array(y_c)
            # sort the x_c and y_c data points, then store them
            x_c_order = np.argsort(x_c)
            self.x_c = x_c[x_c_order]
            self.y_c = y_c[x_c_order]
            # store the number of constraints
            self.c_n = len(self.x_c)

    def calc_slopes(self):
        r&#34;&#34;&#34;
        Calculate the slopes of the lines after a piecewise linear
        function has been fitted.

        This will also calculate the y-intercept from each line in the form
        y = mx + b. The intercepts are stored at self.intercepts.

        Attributes
        ----------
        slopes : ndarray (1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.
        intercepts : ndarray (1-D)
            The y-intercept of each line segment as a 1-D numpy array.

        Returns
        -------
        slopes : ndarray(1-D)
            The slope of each ling segment as a 1-D numpy array. This assumes
            that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
            of the first line segment.

        Examples
        --------
        Calculate the slopes after performing a simple fit

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fit(3)
        &gt;&gt;&gt; slopes = my_pwlf.slopes()

        &#34;&#34;&#34;
        y_hat = self.predict(self.fit_breaks)
        self.slopes = np.zeros(self.n_segments)
        for i in range(self.n_segments):
            self.slopes[i] = (y_hat[i+1]-y_hat[i]) / \
                        (self.fit_breaks[i+1]-self.fit_breaks[i])
        self.intercepts = y_hat[0:-1] - self.slopes*self.fit_breaks[0:-1]
        return self.slopes

    def standard_errors(self):
        r&#34;&#34;&#34;
        Calculate the standard errors for each beta parameter determined
        from the piecewise linear fit. Typically +- 1.96*se will yield the
        center of a 95% confidence region around your parameters. This
        assumes the parmaters follow a normal distribution. For more
        information see:
        https://en.wikipedia.org/wiki/Standard_error

        This calculation follows the derivation provided in [1]_. A taylor-
        series expansion is not needed since this is linear regression.

        Returns
        -------
        se : ndarray (1-D)
            Standard errors associated with each beta parameter. Specifically
            se[0] correspounds to the standard error for beta[0], and so forth.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        Note, this assumes no uncertainty in break point locations.

        References
        ----------
        .. [1] Coppe, A., Haftka, R. T., and Kim, N. H., “Uncertainty
            Identification of Damage Growth Parameters Using Nonlinear
            Regression,” AIAA Journal, Vol. 49, No. 12, dec 2011, pp.
            2818–2821.

        Examples
        --------
        Calculate the standard errors after performing a simple fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; se = my_pwlf.standard_errors()

        &#34;&#34;&#34;
        try:
            nb = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)

        ny = len(self.y_data)

        # initialize the regression matrix as zeros
        A = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

        # try to solve for the standard errors
        try:

            y_hat = np.dot(A, self.beta)
            e = y_hat - self.y_data

            # solve for the unbiased estimate of variance
            variance = np.dot(e, e) / (ny - nb)

            self.se = np.sqrt(variance * (np.linalg.inv(np.dot(A.T,
                                                               A)).diagonal()))

            return self.se

        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def prediction_variance(self, x, sorted_data=False):
        r&#34;&#34;&#34;
        Calculate the prediction variance for each specified x location. The
        prediction variance is the uncertainty of the model due to the lack of
        data. This can be used to find a 95% confidence interval of possible
        piecewise linear models based on the current data. This would be
        done typically as y_hat +- 1.96*np.sqrt(pre_var). The
        prediction_variance needs to be calculated at various x locations.
        For more information see:
        www2.mae.ufl.edu/haftka/vvuq/lectures/Regression-accuracy.pptx

        Parameters
        ----------
        x : array_like
            The x locations where you want the prediction variance from the
            fitted continuous piecewise linear function.
        sorted_data : bool, optional
            Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
            This implentation takes advantage of sorted x data in order to
            speed up the assembly of the regression matrix. A processes that
            could be repeated several thousand times. If your data is not
            sorted, pwlf will use numpy to sort the data. Default is False.

        Returns
        -------
        pre_var : ndarray (1-D)
            Numpy array (floats) of prediction variance at each x location.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Notes
        -----
        This assumes that your break point locations are exact! and does
        not consider the uncertainty with your break point locations.

        Examples
        --------
        Calculate the prediction variance at x_new after performing a simple
        fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; pre_var = my_pwlf.prediction_variance(x_new)

        see also examples/prediction_variance.py

        &#34;&#34;&#34;
        try:
            nb = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)

        ny = len(self.y_data)

        # check if x is numpy array, if not convert to numpy array
        if isinstance(x, np.ndarray) is False:
            x = np.array(x)

        # it is assumed by default that initial arrays are not sorted
        # i.e. if your data is already ordered
        # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
        if sorted_data is False:
            # sort the data from least x to max x
            order_arg = np.argsort(x)
            x = x[order_arg]

        # calculate the prediction variance
        Ad = np.zeros((self.n_data, self.n_parameters))
        # The first two columns of the matrix are always defined as
        Ad[:, 0] = 1.0
        Ad[:, 1] = self.x_data - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
            # only change the non-zero values of A
            Ad[int_index:, i+2] = self.x_data[int_index:] - \
                self.fit_breaks[i+1]

        # try to solve for the unbiased variance estimation
        try:

            y_hat = np.dot(Ad, self.beta)
            e = y_hat - self.y_data

            # solve for the unbiased estimate of variance
            variance = np.dot(e, e) / (ny - nb)

        except np.linalg.LinAlgError:
            raise(&#34;Unable to calculate prediction variance.&#34;
                  &#34; Something went wrong.&#34;)

        # initialize the regression matrix as zeros
        A = np.zeros((len(x), self.n_parameters))
        # The first two columns of the matrix are always defined as
        A[:, 0] = 1.0
        A[:, 1] = x - self.fit_breaks[0]
        # Loop through the rest of A to determine the other columns
        for i in range(self.n_segments-1):
            # find the locations where x &gt; break point values
            int_locations = x &gt; self.fit_breaks[i+1]
            if sum(int_locations) &gt; 0:
                # this if statement just ensures that there is at least
                # one data point in x_c &gt; breaks[i+1]
                # find the first index of x where it is greater than the break
                # point value
                int_index = np.argmax(int_locations)
                # only change the non-zero values of A
                A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

        # try to solve for the prediction variance at the x locations
        try:
            pre_var = variance * \
                np.dot(np.dot(A, np.linalg.inv(np.dot(Ad.T, Ad))), A.T)
            return pre_var.diagonal()

        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def r_squared(self):
        r&#34;&#34;&#34;
        Calculate the coefficient of determination (&#34;R squared&#34;, R^2) value
        after a fit has been performed.
        For more information see:
        https://en.wikipedia.org/wiki/Coefficient_of_determination

        Returns
        -------
        rsq : float
            Coefficient of determination, or &#39;R squared&#39; value.

        Raises
        ------
        ValueError
            You have probably not performed a fit yet.
        LinAlgError
            This typically means your regression problem is ill-conditioned.

        Examples
        --------
        Calculate the R squared value after performing a simple fit.

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; rsq = PiecewiseLinFit.r_squared()

        &#34;&#34;&#34;
        try:
            fit_breaks = self.fit_breaks
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)
        ssr = self.fit_with_breaks(fit_breaks)
        ybar = np.ones(self.n_data) * np.mean(self.y_data)
        ydiff = self.y_data - ybar
        try:
            sst = np.dot(ydiff, ydiff)
            rsq = 1.0 - (ssr/sst)
            return rsq
        except np.linalg.LinAlgError:
            raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)

    def p_values(self):
        r&#34;&#34;&#34;
        Calculate the p-values for each beta parameter.

        This calculates the p-values for the beta parameters under the
        assumption that your break point locations are known. Section 2.4.2 of
        [1]_ defines how to calculate the p-value of individual parameters.
        This is really a marginal test since each parameter is dependent upon
        the other parameters.

        Returns
        -------
        p : ndarray (1-D)
            p-values for each beta parameter where p-value[0] corresponds to
            beta[0] and so forth

        Raises
        ValueError
            You have probably not performed a fit yet.

        Notes
        -----
        This assumes that your break point locations are exact! and does
        not consider the uncertainty with your break point locations.

        See https://github.com/cjekel/piecewise_linear_fit_py/issues/14

        References
        ----------
        .. [1] Myers RH, Montgomery DC, Anderson-Cook CM. Response surface
            methodology . Hoboken. New Jersey: John Wiley &amp; Sons, Inc.
            2009;20:38-44.

        Examples
        --------
        After performing a fit, one can calculate the p-value for each beta
        parameter

        &gt;&gt;&gt; import pwlf
        &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
        &gt;&gt;&gt; y = np.random.random(10)
        &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
        &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
        &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
        &gt;&gt;&gt; p = my_pwlf.p_values(x_new)

        see also examples/standard_errrors_and_p-values.py

        &#34;&#34;&#34;
        # calculate the standard errors associated with each beta parameter
        # not that these standard errors and p-values are only meaningful if
        # you have specified the specific line segment end locations
        # at least for now...
        self.standard_errors()

        # calculate my t-value
        t = self.beta / self.se

        # degrees of freedom for t-distribution
        n = self.n_data
        try:
            k = len(self.beta)
        except ValueError:
            errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                     &#39; a fit before using standard_errors().&#39;
            raise ValueError(errmsg)
        # calculate the p-values
        p = stats.t.sf(np.abs(t), df=n-k-1)
        return p}</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pwlf.pwlf.PiecewiseLinFit.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, x, y, disp_res=False, sorted_data=False)</span>
</code></dt>
<dd>
<section class="desc"><p>An object to fit a continuous piecewise linear function
to data.</p>
<p>Initiate the library with the supplied x and y data.
Supply the x and y data of which you'll be fitting
a continuous piecewise linear model to where y(x).
by default pwlf won't print the optimization results.;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x or independent data point locations as list or 1 dimensional
numpy array. The x and y data should be ordered such that x[i]
corresponds to y[i], for an arbitrary index i.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The y or dependent data point locations as list or 1 dimensional
numpy array.</dd>
<dt><strong><code>disp_res</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether the optimization results should be printed. Default is
False.</dd>
<dt><strong><code>sorted_data</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Data needs to be sorted such that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n-1].
This implementation takes advantage of sorted x data in order to
speed up the assembly of the regression matrix. A process that
could be repeated several thousand times. If your data is not
sorted, pwlf will use numpy to sort the data. Default is False.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>x_data</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The inputted parameter x from the 1-D data set.</dd>
<dt><strong><code>y_data</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The inputted parameter y from the 1-D data set.</dd>
<dt><strong><code>n_data</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of data points.</dd>
<dt><strong><code>break_0</code></strong> :&ensp;<code>float</code></dt>
<dd>The smallest x value.</dd>
<dt><strong><code>break_n</code></strong> :&ensp;<code>float</code></dt>
<dd>The largest x value.</dd>
<dt><strong><code>print</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the optimization results should be printed. Default is
False.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>fit(n_segments, x_c=None, y_c=None, <strong>kwargs)
Fit a continuous piecewise linear function for a specified number
of line segments.
fitfast(n_segments, pop=2, </strong>kwargs)
Fit a continuous piecewise linear function for a specified number
of line segments using a specialized optimization routine that
should be faster than fit() for large problems. The tradeoff may
be that fitfast() results in a lower quality model.
fit_with_breaks(breaks)
Fit a continuous piecewise linear function where the break point
locations are known.
fit_with_breaks_force_points(breaks, x_c, y_c)
Fit a continuous piecewise linear function where the break point
locations are known, and force the fit to go through points at x_c
and y_c.
predict(x, sorted_data=False, beta=None, breaks=None)
Evaluate the continuous piecewise linear function at new untested
points.
fit_with_breaks_opt(var)
The objective function to perform a continuous piecewise linear
fit for a specified number of break points. This is to be used
with a custom optimization routine, and after use_custom_opt has
been called.
fit_force_points_opt(var)'
Same as fit_with_breaks_opt(var), except this allows for points to
be forced through x_c and y_c.
use_custom_opt(n_segments, x_c=None, y_c=None)
Function to initialize the attributes necessary to use a custom
optimization routine. Must be used prior to calling
fit_with_breaks_opt() or fit_force_points_opt().
calc_slopes()
Calculate the slopes of the lines after a piecewise linear
function has been fitted.
standard_errors()
Calculate the standard error of each model parameter in the fitted
piecewise linear function. Note, this assumes no uncertainty in
break point locations.
prediction_variance(x, sorted_data=True)
Calculate the prediction variance at x locations for the fitted
piecewise linear function. Note, assumes no uncertainty in break
point locations.
r_squared()
Calculate the coefficient of determination, or 'R-squared' value
for a fitted piecewise linear function.</p>
<h2 id="examples">Examples</h2>
<p>Initialize for x, y data</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
</code></pre>
<p>Initialize for x,y data and print optimization results</p>
<pre><code>&gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, disp_res=True)
</code></pre>
<p>If your data is already sorted such that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n-1],
use sorted_data=True for a slight performance increase while
initializing the object</p>
<blockquote>
<blockquote>
<blockquote>
<p>my_pWLF = pwlf.PiecewiseLinFit(x, y, sorted_data=True)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, x, y, disp_res=False, sorted_data=False):
    r&#34;&#34;&#34;
    An object to fit a continuous piecewise linear function
    to data.

    Initiate the library with the supplied x and y data.
    Supply the x and y data of which you&#39;ll be fitting
    a continuous piecewise linear model to where y(x).
    by default pwlf won&#39;t print the optimization results.;

    Parameters
    ----------
    x : array_like
        The x or independent data point locations as list or 1 dimensional
        numpy array. The x and y data should be ordered such that x[i]
        corresponds to y[i], for an arbitrary index i.
    y : array_like
        The y or dependent data point locations as list or 1 dimensional
        numpy array.
    disp_res : bool, optional
        Whether the optimization results should be printed. Default is
        False.
    sorted_data : bool, optional
        Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
        This implementation takes advantage of sorted x data in order to
        speed up the assembly of the regression matrix. A process that
        could be repeated several thousand times. If your data is not
        sorted, pwlf will use numpy to sort the data. Default is False.

    Attributes
    --------
    x_data : ndarray (1-D)
        The inputted parameter x from the 1-D data set.
    y_data : ndarray (1-D)
        The inputted parameter y from the 1-D data set.
    n_data : int
        The number of data points.
    break_0 : float
        The smallest x value.
    break_n : float
        The largest x value.
    print : bool
        Whether the optimization results should be printed. Default is
        False.

    Methods
    -------
    fit(n_segments, x_c=None, y_c=None, **kwargs)
        Fit a continuous piecewise linear function for a specified number
        of line segments.
    fitfast(n_segments, pop=2, **kwargs)
        Fit a continuous piecewise linear function for a specified number
        of line segments using a specialized optimization routine that
        should be faster than fit() for large problems. The tradeoff may
        be that fitfast() results in a lower quality model.
    fit_with_breaks(breaks)
        Fit a continuous piecewise linear function where the break point
        locations are known.
    fit_with_breaks_force_points(breaks, x_c, y_c)
        Fit a continuous piecewise linear function where the break point
        locations are known, and force the fit to go through points at x_c
        and y_c.
    predict(x, sorted_data=False, beta=None, breaks=None)
        Evaluate the continuous piecewise linear function at new untested
        points.
    fit_with_breaks_opt(var)
        The objective function to perform a continuous piecewise linear
        fit for a specified number of break points. This is to be used
        with a custom optimization routine, and after use_custom_opt has
        been called.
    fit_force_points_opt(var)&#39;
        Same as fit_with_breaks_opt(var), except this allows for points to
        be forced through x_c and y_c.
    use_custom_opt(n_segments, x_c=None, y_c=None)
        Function to initialize the attributes necessary to use a custom
        optimization routine. Must be used prior to calling
        fit_with_breaks_opt() or fit_force_points_opt().
    calc_slopes()
        Calculate the slopes of the lines after a piecewise linear
        function has been fitted.
    standard_errors()
        Calculate the standard error of each model parameter in the fitted
        piecewise linear function. Note, this assumes no uncertainty in
        break point locations.
    prediction_variance(x, sorted_data=True)
        Calculate the prediction variance at x locations for the fitted
        piecewise linear function. Note, assumes no uncertainty in break
        point locations.
    r_squared()
        Calculate the coefficient of determination, or &#39;R-squared&#39; value
        for a fitted piecewise linear function.

    Examples
    --------
    Initialize for x, y data

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)

    Initialize for x,y data and print optimization results

    &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, disp_res=True)

    If your data is already sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1],
    use sorted_data=True for a slight performance increase while
    initializing the object

    &gt;&gt;&gt; my_pWLF = pwlf.PiecewiseLinFit(x, y, sorted_data=True)
    &#34;&#34;&#34;

    self.print = disp_res

    # x and y should be numpy arrays
    # if they are not convert to numpy array
    if isinstance(x, np.ndarray) is False:
        x = np.array(x)
    if isinstance(y, np.ndarray) is False:
        y = np.array(y)

    self.sorted_data = sorted_data

    # it is assumed by default that initial arrays are not sorted
    # i.e. if your data is already ordered
    # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
    if self.sorted_data:
        self.x_data = x
        self.y_data = y
    else:
        # sort the data from least x to max x
        order_arg = np.argsort(x)
        self.x_data = x[order_arg]
        self.y_data = y[order_arg]
    # calculate the number of data points
    self.n_data = len(x)

    # set the first and last break x values to be the min and max of x
    self.break_0 = np.min(self.x_data)
    self.break_n = np.max(self.x_data)}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.calc_slopes"><code class="name flex">
<span>def <span class="ident">calc_slopes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the slopes of the lines after a piecewise linear
function has been fitted.</p>
<p>This will also calculate the y-intercept from each line in the form
y = mx + b. The intercepts are stored at self.intercepts.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
<dt><strong><code>intercepts</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The y-intercept of each line segment as a 1-D numpy array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code>(<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Calculate the slopes after performing a simple fit</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fit(3)
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>slopes = my_pwlf.slopes()</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def calc_slopes(self):
    r&#34;&#34;&#34;
    Calculate the slopes of the lines after a piecewise linear
    function has been fitted.

    This will also calculate the y-intercept from each line in the form
    y = mx + b. The intercepts are stored at self.intercepts.

    Attributes
    ----------
    slopes : ndarray (1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.
    intercepts : ndarray (1-D)
        The y-intercept of each line segment as a 1-D numpy array.

    Returns
    -------
    slopes : ndarray(1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.

    Examples
    --------
    Calculate the slopes after performing a simple fit

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fit(3)
    &gt;&gt;&gt; slopes = my_pwlf.slopes()

    &#34;&#34;&#34;
    y_hat = self.predict(self.fit_breaks)
    self.slopes = np.zeros(self.n_segments)
    for i in range(self.n_segments):
        self.slopes[i] = (y_hat[i+1]-y_hat[i]) / \
                    (self.fit_breaks[i+1]-self.fit_breaks[i])
    self.intercepts = y_hat[0:-1] - self.slopes*self.fit_breaks[0:-1]
    return self.slopes}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, n_segments, x_c=None, y_c=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit a continuous piecewise linear function for a specified number
of line segments. Uses differential evolution to finds the optimum
location of break points for a given number of line segments by
minimizing the sum of the square error.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The desired number of line segments.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>array_like</code>, optional</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>optional</code></dt>
<dd>Directly passed into scipy.optimize.differential_evolution(). This
will override any pwlf defaults when provided. See Note for more
information.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>ssr</code></strong> :&ensp;<code>float</code></dt>
<dd>Optimal sum of square error.</dd>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>nVar</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of variables in the global optimization problem.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters for the continuous piecewise linear fit.</dd>
<dt><strong><code>zeta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters associated with the constraint function,
if x_c and y_c is provided. Only created if x_c and y_c provided.</dd>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through. Only created if x_c
and y_c provided.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through. Only created if x_c
and y_c provided.</dd>
<dt><strong><code>c_n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of constraint points. This is the same as len(x_c).
Only created if x_c and y_c provided.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>float</code></dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>You probably provided x_c without y_c (or vice versa).
You must provide both x_c and y_c if you plan to force
the model through data point(s).</dd>
</dl>
<h2 id="notes">Notes</h2>
<dl>
<dt>All **kwargs are passed into sicpy.optimize.differential_evolution.</dt>
<dt>If any **kwargs is used, it will override my differential_evolution,</dt>
<dt>defaults. This allows advanced users to tweak their own optimization.</dt>
<dt>For me information see:</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>github.com</code>/<code>cjekel</code>/<code>piecewise_linear_fit_py</code>/<code>issues</code>/<code>15</code>#<code>issuecomment</code>-<code>434717232</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>This example shows you how to fit three continuous piecewise lines to
a dataset. This assumes that x is linearly spaced from [0, 1), and y is
random.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fit(3)
</code></pre>
<p>Additionally you desired that the piecewise linear function go
through the point (0.0, 0.0).</p>
<pre><code>&gt;&gt;&gt; x_c = [0.0]
&gt;&gt;&gt; y_c = [0.0]
&gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)
</code></pre>
<p>Additionally you desired that the piecewise linear function go
through the points (0.0, 0.0) and (1.0, 1.0).</p>
<pre><code>&gt;&gt;&gt; x_c = [0.0, 1.0]
&gt;&gt;&gt; y_c = [0.0, 1.0]
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit(self, n_segments, x_c=None, y_c=None, **kwargs):
    r&#34;&#34;&#34;
    Fit a continuous piecewise linear function for a specified number
    of line segments. Uses differential evolution to finds the optimum
    location of break points for a given number of line segments by
    minimizing the sum of the square error.

    Parameters
    ----------
    n_segments : int
        The desired number of line segments.
    x_c : array_like, optional
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    y_c : array_like, optional
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    **kwargs : optional
        Directly passed into scipy.optimize.differential_evolution(). This
        will override any pwlf defaults when provided. See Note for more
        information.

    Attributes
    ----------
    ssr : float
        Optimal sum of square error.
    fit_breaks : ndarray (1-D)
        Break point locations stored as a 1-D numpy array.
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    n_segments : int
        The number of line segments.
    nVar : int
        The number of variables in the global optimization problem.
    beta : ndarray (1-D)
        The model parameters for the continuous piecewise linear fit.
    zeta : ndarray (1-D)
        The model parameters associated with the constraint function,
        if x_c and y_c is provided. Only created if x_c and y_c provided.
    slopes : ndarray (1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.
    x_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through. Only created if x_c
        and y_c provided.
    y_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through. Only created if x_c
        and y_c provided.
    c_n : int
        The number of constraint points. This is the same as len(x_c).
        Only created if x_c and y_c provided.

    Returns
    -------
    fit_breaks : float
        Break point locations stored as a 1-D numpy array.

    Raises
    ------
    ValueError
        You probably provided x_c without y_c (or vice versa).
        You must provide both x_c and y_c if you plan to force
        the model through data point(s).

    Notes
    -----
    All **kwargs are passed into sicpy.optimize.differential_evolution.
    If any **kwargs is used, it will override my differential_evolution,
    defaults. This allows advanced users to tweak their own optimization.
    For me information see:
    https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

    Examples
    --------
    This example shows you how to fit three continuous piecewise lines to
    a dataset. This assumes that x is linearly spaced from [0, 1), and y is
    random.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fit(3)

    Additionally you desired that the piecewise linear function go
    through the point (0.0, 0.0).

    &gt;&gt;&gt; x_c = [0.0]
    &gt;&gt;&gt; y_c = [0.0]
    &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

    Additionally you desired that the piecewise linear function go
    through the points (0.0, 0.0) and (1.0, 1.0).

    &gt;&gt;&gt; x_c = [0.0, 1.0]
    &gt;&gt;&gt; y_c = [0.0, 1.0]
    &gt;&gt;&gt; breaks = my_pwlf.fit(3, x_c=x_c, y_c=y_c)

    &#34;&#34;&#34;

    # check to see if you&#39;ve provided just x_c or y_c
    logic1 = x_c is not None and y_c is None
    logic2 = y_c is not None and x_c is None
    if logic1 or logic2:
        raise ValueError(&#39;You must provide both x_c and y_c!&#39;)

    # set the function to minimize
    min_function = self.fit_with_breaks_opt

    # if you&#39;ve provided both x_c and y_c
    if x_c is not None and y_c is not None:
        # check if x_c and y_c are numpy array
        # if not convert to numpy array
        if isinstance(x_c, np.ndarray) is False:
            x_c = np.array(x_c)
        if isinstance(y_c, np.ndarray) is False:
            y_c = np.array(y_c)
        # sort the x_c and y_c data points, then store them
        x_c_order = np.argsort(x_c)
        self.x_c = x_c[x_c_order]
        self.y_c = y_c[x_c_order]
        # store the number of constraints
        self.c_n = len(self.x_c)
        # Use a different function to minimize
        min_function = self.fit_force_points_opt

    # store the number of line segments and number of parameters
    self.n_segments = int(n_segments)
    self.n_parameters = self.n_segments + 1

    # calculate the number of variables I have to solve for
    self.nVar = self.n_segments - 1

    # initiate the bounds of the optimization
    bounds = np.zeros([self.nVar, 2])
    bounds[:, 0] = self.break_0
    bounds[:, 1] = self.break_n

    # run the optimization
    if len(kwargs) == 0:
        res = differential_evolution(min_function, bounds,
                                     strategy=&#39;best1bin&#39;, maxiter=1000,
                                     popsize=50, tol=1e-3,
                                     mutation=(0.5, 1), recombination=0.7,
                                     seed=None, callback=None, disp=False,
                                     polish=True, init=&#39;latinhypercube&#39;,
                                     atol=1e-4)
    else:
        res = differential_evolution(min_function,
                                     bounds, **kwargs)
    if self.print is True:
        print(res)

    self.ssr = res.fun

    # pull the breaks out of the result
    var = np.sort(res.x)
    breaks = np.zeros(len(var) + 2)
    breaks[1:-1] = var.copy()
    breaks[0] = self.break_0
    breaks[-1] = self.break_n

    # assign values
    if x_c is None and y_c is None:
        self.fit_with_breaks(breaks)
    else:
        self.fit_with_breaks_force_points(breaks, self.x_c, self.y_c)

    return self.fit_breaks}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fit_force_points_opt"><code class="name flex">
<span>def <span class="ident">fit_force_points_opt</span></span>(<span>self, var)</span>
</code></dt>
<dd>
<section class="desc"><p>The objective function to perform a continuous piecewise linear
fit for a specified number of break points. This is to be used
with a custom optimization routine, and after use_custom_opt has
been called.</p>
<p>Use this function if you intend to be force the model through
x_c and y_c, while performing a custom optimization.</p>
<p>This was intended for advanced users only.</p>
<dl>
<dt>See the following example</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>github.com</code>/<code>cjekel</code>/<code>piecewise_linear_fit_py</code>/<code>blob</code>/<code>master</code>/<code>examples</code>/<code>useCustomOptimizationRoutine.py</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>var</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The break point locations, or variable, in a custom
optimization routine.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ssr</code></strong> :&ensp;<code>float</code></dt>
<dd>The sum of square of the residuals.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>You should run use_custom_opt to initialize necessary object
attributes first.</p>
<p>Unlike fit_with_breaks_force_points, fit_force_points_opt
automatically assumes that the first and last break points occur
at the min and max values of x.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_force_points_opt(self, var):
    r&#34;&#34;&#34;
    The objective function to perform a continuous piecewise linear
    fit for a specified number of break points. This is to be used
    with a custom optimization routine, and after use_custom_opt has
    been called.

    Use this function if you intend to be force the model through
    x_c and y_c, while performing a custom optimization.

    This was intended for advanced users only.

    See the following example
    https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

    Parameters
    ----------
    var : array_like
        The break point locations, or variable, in a custom
        optimization routine.

    Returns
    -------
    ssr : float
        The sum of square of the residuals.

    Raises
    ------
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    You should run use_custom_opt to initialize necessary object
    attributes first.

    Unlike fit_with_breaks_force_points, fit_force_points_opt
    automatically assumes that the first and last break points occur
    at the min and max values of x.
    &#34;&#34;&#34;

    var = np.sort(var)
    breaks = np.zeros(len(var) + 2)
    breaks[1:-1] = var.copy()
    breaks[0] = self.break_0
    breaks[-1] = self.break_n

    # Sort the breaks, then store them
    breaks_order = np.argsort(breaks)
    breaks = breaks[breaks_order]

    # initialize the regression matrix as zeros
    A = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = self.x_data - breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; breaks[i+1])
        # only change the non-zero values of A
        A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

    # Assemble the constraint matrix
    C = np.zeros((self.c_n, self.n_parameters))
    C[:, 0] = 1.0
    C[:, 1] = self.x_c - breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the locations where x &gt; break point values
        int_locations = self.x_c &gt; breaks[i+1]
        if sum(int_locations) &gt; 0:
            # this if statement just ensures that there is at least
            # one data point in x_c &gt; breaks[i+1]
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(int_locations)
            # only change the non-zero values of A
            C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

    # Assemble the square constrained least squares matrix
    K = np.zeros((self.n_parameters + self.c_n,
                  self.n_parameters + self.c_n))
    K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
    K[:self.n_parameters, self.n_parameters:] = C.T
    K[self.n_parameters:, :self.n_parameters] = C
    # Assemble right hand side vector
    yt = np.dot(2.0*A.T, self.y_data)
    z = np.zeros(self.n_parameters + self.c_n)
    z[:self.n_parameters] = yt
    z[self.n_parameters:] = self.y_c

    # try to solve the regression problem
    try:
        # Solve the least squares problem
        beta_prime = np.linalg.solve(K, z)

        # save the beta parameters
        self.beta = beta_prime[0:self.n_parameters]
        # save the zeta parameters
        self.zeta = beta_prime[self.n_parameters:]

        # Calculate ssr
        # where ssr = sum of square of residuals
        y_hat = np.dot(A, self.beta)
        e = y_hat - self.y_data
        ssr = np.dot(e, e)

        # Calculate the Lagrangian function
        p = np.dot(C.T, self.zeta)
        L = np.sum(np.abs(p)) + ssr

    except np.linalg.LinAlgError:
        # the computation could not converge!
        # on an error, return L = np.inf
        # You might have a singular Matrix!!!
        L = np.inf
    if L is None:
        L = np.inf
        # something went wrong...
    return L}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks"><code class="name flex">
<span>def <span class="ident">fit_with_breaks</span></span>(<span>self, breaks)</span>
</code></dt>
<dd>
<section class="desc"><p>A function which fits a continuous piecewise linear function
for specified break point locations.</p>
<p>The function minimizes the sum of the square of the residuals for the
x y data.</p>
<dl>
<dt>If you want to understand the math behind this read</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>jekel.me</code>/<code>2018</code>/<code>Continous</code>-<code>piecewise</code>-<code>linear</code>-<code>regression</code>/</dt>
<dd>&nbsp;</dd>
<dt>Other useful resources:</dt>
<dt><strong><code>http</code></strong> :&ensp;//<code>golovchenko.org</code>/<code>docs</code>/<code>ContinuousPiecewiseLinearFit.pdf</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>https</code></strong> :&ensp;//<code>www.mathworks.com</code>/<code>matlabcentral</code>/<code>fileexchange</code>/<code>40913</code>-<code>piecewise</code>-<code>linear</code>-<code>least</code>-<code>square</code>-<code>fit</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>http</code></strong> :&ensp;//<code>www.regressionist.com</code>/<code>2018</code>/<code>02</code>/<code>07</code>/<code>continuous</code>-<code>piecewise</code>-<code>linear</code>-<code>fitting</code>/</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>breaks</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations where each line segment terminates. These are
referred to as break points for each line segment. This should be
structured as a 1-D numpy array.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters for the continuous piecewise linear fit.</dd>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ssr</code></strong> :&ensp;<code>float</code></dt>
<dd>Returns the sum of squares of the residuals.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The above attributes are added or modified while running this function.</p>
<h2 id="examples">Examples</h2>
<p>If your x data exists from 0 &lt;= x &lt;= 1 and you want three
piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
random.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>ssr = my_pwlf.fit_with_breaks(breaks)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_with_breaks(self, breaks):
    r&#34;&#34;&#34;
    A function which fits a continuous piecewise linear function
    for specified break point locations.

    The function minimizes the sum of the square of the residuals for the
    x y data.

    If you want to understand the math behind this read
    https://jekel.me/2018/Continous-piecewise-linear-regression/

    Other useful resources:
    http://golovchenko.org/docs/ContinuousPiecewiseLinearFit.pdf
    https://www.mathworks.com/matlabcentral/fileexchange/40913-piecewise-linear-least-square-fit
    http://www.regressionist.com/2018/02/07/continuous-piecewise-linear-fitting/

    Parameters
    ----------
    breaks : array_like
        The x locations where each line segment terminates. These are
        referred to as break points for each line segment. This should be
        structured as a 1-D numpy array.

    Attributes
    ----------
    fit_breaks : ndarray (1-D)
        Break point locations stored as a 1-D numpy array.
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    n_segments : int
        The number of line segments.
    beta : ndarray (1-D)
        The model parameters for the continuous piecewise linear fit.
    slopes : ndarray (1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.

    Returns
    -------
    ssr : float
        Returns the sum of squares of the residuals.

    Raises
    ------
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    The above attributes are added or modified while running this function.

    Examples
    --------
    If your x data exists from 0 &lt;= x &lt;= 1 and you want three
    piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
    and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
    random.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
    &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)

    &#34;&#34;&#34;

    # Check if breaks in ndarray, if not convert to np.array
    if isinstance(breaks, np.ndarray) is False:
        breaks = np.array(breaks)

    # Sort the breaks, then store them
    breaks_order = np.argsort(breaks)
    self.fit_breaks = breaks[breaks_order]
    # store the number of parameters and line segments
    self.n_parameters = len(breaks)
    self.n_segments = self.n_parameters - 1

    # initialize the regression matrix as zeros
    A = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = self.x_data - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
        # only change the non-zero values of A
        A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

    # try to solve the regression problem
    try:
        # least squares solver
        beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)
        # save the beta parameters
        self.beta = beta

        # save the slopes
        self.calc_slopes()

        # ssr is only calculated if self.n_data &gt; self.n_parameters
        # in this case I&#39;ll need to calculate ssr manually
        # where ssr = sum of square of residuals
        if self.n_data &lt;= self.n_parameters:
            y_hat = np.dot(A, beta)
            e = y_hat - self.y_data
            ssr = [np.dot(e, e)]

        # if ssr still hasn&#39;t been calculated... Then try again
        if len(ssr) == 0:
            y_hat = np.dot(A, beta)
            e = y_hat - self.y_data
            ssr = [np.dot(e, e)]

    except np.linalg.LinAlgError:
        # the computation could not converge!
        # on an error, return ssr = np.print_function
        # You might have a singular Matrix!!!
        ssr = np.inf
    if ssr is None:
        ssr = np.inf
        # something went wrong...
    return ssr[0]}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_force_points"><code class="name flex">
<span>def <span class="ident">fit_with_breaks_force_points</span></span>(<span>self, breaks, x_c, y_c)</span>
</code></dt>
<dd>
<section class="desc"><p>A function which fits a continuous piecewise linear function
for specified break point locations, where you force the
fit to go through the data points at x_c and y_c.</p>
<p>The function minimizes the sum of the square of the residuals for the
pair of x, y data points.</p>
<dl>
<dt>If you want to understand the math behind this read</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>jekel.me</code>/<code>2018</code>/<code>Force</code>-<code>piecwise</code>-<code>linear</code>-<code>fit</code>-<code>through</code>-<code>data</code>/</dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>breaks</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations where each line segment terminates. These are
referred to as break points for each line segment. This should be
structured as a 1-D numpy array.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters for the continuous piecewise linear fit.</dd>
<dt><strong><code>zeta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters associated with the constraint function.</dd>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>c_n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of constraint points. This is the same as len(x_c).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>L</code></strong> :&ensp;<code>float</code></dt>
<dd>Returns the Lagrangian function value. This is the sum of squares
of the residuals plus the constraint penalty.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The above attributes are added or modified while running this function.
Input:</p>
<h2 id="examples">Examples</h2>
<p>If your x data exists from 0 &lt;= x &lt;= 1 and you want three
piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
random. Additionally you desired that the piecewise linear function go
through the point (0.0, 0.0)</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; x_c = [0.0]
&gt;&gt;&gt; y_c = [0.0]
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>L = my_pwlf.fit_with_breaks_force_points(breaks, x_c, y_c)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_with_breaks_force_points(self, breaks, x_c, y_c):
    r&#34;&#34;&#34;
    A function which fits a continuous piecewise linear function
    for specified break point locations, where you force the
    fit to go through the data points at x_c and y_c.

    The function minimizes the sum of the square of the residuals for the
    pair of x, y data points.

    If you want to understand the math behind this read
    https://jekel.me/2018/Force-piecwise-linear-fit-through-data/

    Parameters
    ----------
    breaks : array_like
        The x locations where each line segment terminates. These are
        referred to as break points for each line segment. This should be
        structured as a 1-D numpy array.
    x_c : array_like
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    y_c : array_like
        The x locations of the data points that the piecewise linear
        function will be forced to go through.

    Attributes
    ----------
    fit_breaks : ndarray (1-D)
        Break point locations stored as a 1-D numpy array.
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    n_segments : int
        The number of line segments.
    beta : ndarray (1-D)
        The model parameters for the continuous piecewise linear fit.
    zeta : ndarray (1-D)
        The model parameters associated with the constraint function.
    slopes : ndarray (1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.
    x_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    y_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    c_n : int
        The number of constraint points. This is the same as len(x_c).


    Returns
    -------
    L : float
        Returns the Lagrangian function value. This is the sum of squares
        of the residuals plus the constraint penalty.

    Raises
    ------
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    The above attributes are added or modified while running this function.
    Input:

    Examples
    -------
    If your x data exists from 0 &lt;= x &lt;= 1 and you want three
    piecewise linear lines where the lines terminate at x = 0.0, 0.3, 0.6,
    and 1.0. This assumes that x is linearly spaced from [0, 1), and y is
    random. Additionally you desired that the piecewise linear function go
    through the point (0.0, 0.0)

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; x_c = [0.0]
    &gt;&gt;&gt; y_c = [0.0]
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
    &gt;&gt;&gt; L = my_pwlf.fit_with_breaks_force_points(breaks, x_c, y_c)

    &#34;&#34;&#34;

    # check if x_c and y_c are numpy array, if not convert to numpy array
    if isinstance(x_c, np.ndarray) is False:
        x_c = np.array(x_c)
    if isinstance(y_c, np.ndarray) is False:
        y_c = np.array(y_c)
    # sort the x_c and y_c data points, then store them
    x_c_order = np.argsort(x_c)
    self.x_c = x_c[x_c_order]
    self.y_c = y_c[x_c_order]
    # store the number of constraints
    self.c_n = len(self.x_c)

    # Check if breaks in ndarray, if not convert to np.array
    if isinstance(breaks, np.ndarray) is False:
        breaks = np.array(breaks)

    # Sort the breaks, then store them
    breaks_order = np.argsort(breaks)
    self.fit_breaks = breaks[breaks_order]
    # store the number of parameters and line segments
    self.n_parameters = len(breaks)
    self.n_segments = self.n_parameters - 1

    # initialize the regression matrix as zeros
    A = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = self.x_data - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
        # only change the non-zero values of A
        A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

    # Assemble the constraint matrix
    C = np.zeros((self.c_n, self.n_parameters))
    C[:, 0] = 1.0
    C[:, 1] = self.x_c - breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the locations where x &gt; break point values
        int_locations = self.x_c &gt; breaks[i+1]
        if sum(int_locations) &gt; 0:
            # this if statement just ensures that there is at least
            # one data point in x_c &gt; breaks[i+1]
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(int_locations)
            # only change the non-zero values of A
            C[int_index:, i+2] = self.x_c[int_index:] - breaks[i+1]

    # Assemble the square constrained least squares matrix
    K = np.zeros((self.n_parameters + self.c_n,
                  self.n_parameters + self.c_n))
    K[0:self.n_parameters, 0:self.n_parameters] = 2.0 * np.dot(A.T, A)
    K[:self.n_parameters, self.n_parameters:] = C.T
    K[self.n_parameters:, :self.n_parameters] = C
    # Assemble right hand side vector
    yt = np.dot(2.0*A.T, self.y_data)
    z = np.zeros(self.n_parameters + self.c_n)
    z[:self.n_parameters] = yt
    z[self.n_parameters:] = self.y_c

    # try to solve the regression problem
    try:
        # Solve the least squares problem
        beta_prime = np.linalg.solve(K, z)

        # save the beta parameters
        self.beta = beta_prime[0:self.n_parameters]
        # save the zeta parameters
        self.zeta = beta_prime[self.n_parameters:]

        # save the slopes
        self.calc_slopes()

        # Calculate ssr
        # where ssr = sum of square of residuals
        y_hat = np.dot(A, self.beta)
        e = y_hat - self.y_data
        ssr = np.dot(e, e)

        # Calculate the Lagrangian function
        # c_x_y = np.dot(C, self.x_c.T) - self.y_c
        p = np.dot(C.T, self.zeta)
        L = np.sum(np.abs(p)) + ssr

    except np.linalg.LinAlgError:
        # the computation could not converge!
        # on an error, return L = np.inf
        # You might have a singular Matrix!!!
        L = np.inf
    if L is None:
        L = np.inf
        # something went wrong...
    return L}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_opt"><code class="name flex">
<span>def <span class="ident">fit_with_breaks_opt</span></span>(<span>self, var)</span>
</code></dt>
<dd>
<section class="desc"><p>The objective function to perform a continuous piecewise linear
fit for a specified number of break points. This is to be used
with a custom optimization routine, and after use_custom_opt has
been called.</p>
<p>This was intended for advanced users only.</p>
<dl>
<dt>See the following example</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>github.com</code>/<code>cjekel</code>/<code>piecewise_linear_fit_py</code>/<code>blob</code>/<code>master</code>/<code>examples</code>/<code>useCustomOptimizationRoutine.py</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>var</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The break point locations, or variable, in a custom
optimization routine.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ssr</code></strong> :&ensp;<code>float</code></dt>
<dd>The sum of square of the residuals.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>You should run use_custom_opt to initialize necessary object
attributes first.</p>
<p>Unlike fit_with_breaks, fit_with_breaks_opt automatically
assumes that the first and last break points occur at the min and max
values of x.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_with_breaks_opt(self, var):
    r&#34;&#34;&#34;
    The objective function to perform a continuous piecewise linear
    fit for a specified number of break points. This is to be used
    with a custom optimization routine, and after use_custom_opt has
    been called.

    This was intended for advanced users only.

    See the following example
    https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

    Parameters
    ----------
    var : array_like
        The break point locations, or variable, in a custom
        optimization routine.

    Returns
    -------
    ssr : float
        The sum of square of the residuals.

    Raises
    ------
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    You should run use_custom_opt to initialize necessary object
    attributes first.

    Unlike fit_with_breaks, fit_with_breaks_opt automatically
    assumes that the first and last break points occur at the min and max
    values of x.
    &#34;&#34;&#34;

    var = np.sort(var)
    breaks = np.zeros(len(var) + 2)
    breaks[1:-1] = var.copy()
    breaks[0] = self.break_0
    breaks[-1] = self.break_n

    # Sort the breaks, then store them
    breaks_order = np.argsort(breaks)
    breaks = breaks[breaks_order]

    # initialize the regression matrix as zeros
    A = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = self.x_data - breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; breaks[i+1])
        # only change the non-zero values of A
        A[int_index:, i+2] = self.x_data[int_index:] - breaks[i+1]

    # try to solve the regression problem
    try:
        # least squares solver
        beta, ssr, rank, s = np.linalg.lstsq(A, self.y_data, rcond=None)

        # ssr is only calculated if self.n_data &gt; self.n_parameters
        # in all other cases I&#39;ll need to calculate ssr manually
        # where ssr = sum of square of residuals
        if self.n_data &lt;= self.n_parameters:
            y_hat = np.dot(A, beta)
            e = y_hat - self.y_data
            ssr = [np.dot(e, e)]

        # if ssr still hasn&#39;t been calculated... Then try again
        if len(ssr) == 0:
            y_hat = np.dot(A, beta)
            e = y_hat - self.y_data
            ssr = [np.dot(e, e)]

    except np.linalg.LinAlgError:
        # the computation could not converge!
        # on an error, return ssr = np.inf
        # You might have a singular Matrix!!!
        ssr = np.inf
    if ssr is None:
        ssr = np.inf
        # something went wrong...
    return ssr[0]}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.fitfast"><code class="name flex">
<span>def <span class="ident">fitfast</span></span>(<span>self, n_segments, pop=2, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Uses multi start LBFGSB optimization to find the location of
break points for a given number of line segments by minimizing the sum
of the square of the errors.</p>
<p>The idea is that we generate n random latin hypercube samples
and run LBFGSB optimization on each one. This isn't guaranteed to
find the global optimum. It's suppose to be a reasonable compromise
between speed and quality of fit. Let me know how it works.</p>
<p>Since this is based on random sampling, you might want to run it
multiple times and save the best version&hellip; The best version will
have the lowest self.ssr (sum of square of residuals).</p>
<p>There is no guarantee that this will be faster than fit(), however
you may find it much faster sometimes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The desired number of line segments.</dd>
<dt><strong><code>pop</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of latin hypercube samples to generate. Default pop=2.</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>optional</code></dt>
<dd>Directly passed into scipy.optimize.differential_evolution(). This
will override any pwlf defaults when provided. See Note for more
information.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>ssr</code></strong> :&ensp;<code>float</code></dt>
<dd>Optimal sum of square error.</dd>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>nVar</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of variables in the global optimization problem.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters for the continuous piecewise linear fit.</dd>
<dt><strong><code>slopes</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The slope of each ling segment as a 1-D numpy array. This assumes
that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n]. Thus, slopes[0] is the slope
of the first line segment.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>float</code></dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The default number of multi start optimizations is 2.
- Decreasing this number will result in a faster run time.
- Increasing this number will improve the likelihood of finding
good results
- You can specify the number of starts using the following call
- Minimum value of pop is 2</p>
<dl>
<dt>All **kwargs are passed into sicpy.optimize.fmin_l_bfgs_b. If any</dt>
<dt>**kwargs is used, it will override my defaults. This allows</dt>
<dt>advanced users to tweak their own optimization. For me information see:</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>github.com</code>/<code>cjekel</code>/<code>piecewise_linear_fit_py</code>/<code>issues</code>/<code>15</code>#<code>issuecomment</code>-<code>434717232</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>This example shows you how to fit three continuous piecewise lines to
a dataset. This assumes that x is linearly spaced from [0, 1), and y is
random.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
</code></pre>
<p>You can change the number of latin hypercube samples (or starting
point, locations) to use with pop. The following example will use 50
samples.</p>
<blockquote>
<blockquote>
<blockquote>
<p>breaks = my_pwlf.fitfast(3, pop=50)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fitfast(self, n_segments, pop=2, **kwargs):
    r&#34;&#34;&#34;
    Uses multi start LBFGSB optimization to find the location of
    break points for a given number of line segments by minimizing the sum
    of the square of the errors.

    The idea is that we generate n random latin hypercube samples
    and run LBFGSB optimization on each one. This isn&#39;t guaranteed to
    find the global optimum. It&#39;s suppose to be a reasonable compromise
    between speed and quality of fit. Let me know how it works.

    Since this is based on random sampling, you might want to run it
    multiple times and save the best version... The best version will
    have the lowest self.ssr (sum of square of residuals).

    There is no guarantee that this will be faster than fit(), however
    you may find it much faster sometimes.

    Parameters
    ----------
    n_segments : int
        The desired number of line segments.
    pop : int, optional
        The number of latin hypercube samples to generate. Default pop=2.
    **kwargs : optional
        Directly passed into scipy.optimize.differential_evolution(). This
        will override any pwlf defaults when provided. See Note for more
        information.

    Attributes
    ----------
    ssr : float
        Optimal sum of square error.
    fit_breaks : ndarray (1-D)
        Break point locations stored as a 1-D numpy array.
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    n_segments : int
        The number of line segments.
    nVar : int
        The number of variables in the global optimization problem.
    beta : ndarray (1-D)
        The model parameters for the continuous piecewise linear fit.
    slopes : ndarray (1-D)
        The slope of each ling segment as a 1-D numpy array. This assumes
        that x[0] &lt;= x[1] &lt;= ... &lt;= x[n]. Thus, slopes[0] is the slope
        of the first line segment.

    Returns
    -------
    fit_breaks : float
        Break point locations stored as a 1-D numpy array.

    Notes
    -----
    The default number of multi start optimizations is 2.
        - Decreasing this number will result in a faster run time.
        - Increasing this number will improve the likelihood of finding
            good results
        - You can specify the number of starts using the following call
        - Minimum value of pop is 2

    All **kwargs are passed into sicpy.optimize.fmin_l_bfgs_b. If any
    **kwargs is used, it will override my defaults. This allows
    advanced users to tweak their own optimization. For me information see:
    https://github.com/cjekel/piecewise_linear_fit_py/issues/15#issuecomment-434717232

    Examples
    --------
    This example shows you how to fit three continuous piecewise lines to
    a dataset. This assumes that x is linearly spaced from [0, 1), and y is
    random.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)

    You can change the number of latin hypercube samples (or starting
    point, locations) to use with pop. The following example will use 50
    samples.

    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3, pop=50)

    &#34;&#34;&#34;
    pop = int(pop)  # ensure that the population is integer

    self.n_segments = int(n_segments)
    self.n_parameters = self.n_segments + 1

    # calculate the number of variables I have to solve for
    self.nVar = self.n_segments - 1

    # initiate the bounds of the optimization
    bounds = np.zeros([self.nVar, 2])
    bounds[:, 0] = self.break_0
    bounds[:, 1] = self.break_n

    # perform latin hypercube sampling
    mypop = lhs(self.nVar, samples=pop, criterion=&#39;maximin&#39;)
    # scale the sampling to my variable range
    mypop = mypop * (self.break_n - self.break_0) + self.break_0

    x = np.zeros((pop, self.nVar))
    f = np.zeros(pop)
    d = []

    for i, x0 in enumerate(mypop):
        if len(kwargs) == 0:
            resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                             fprime=None, args=(),
                                             approx_grad=True,
                                             bounds=bounds, m=10,
                                             factr=1e2, pgtol=1e-05,
                                             epsilon=1e-08, iprint=-1,
                                             maxfun=15000, maxiter=15000,
                                             disp=None, callback=None)
        else:
            resx, resf, resd = fmin_l_bfgs_b(self.fit_with_breaks_opt, x0,
                                             fprime=None, approx_grad=True,
                                             bounds=bounds, **kwargs)
        x[i, :] = resx
        f[i] = resf
        d.append(resd)
        if self.print is True:
            print(i + 1, &#39;of &#39; + str(pop) + &#39; complete&#39;)

    # find the best result
    best_ind = np.nanargmin(f)
    best_val = f[best_ind]
    best_break = x[best_ind]
    res = (x[best_ind], f[best_ind], d[best_ind])
    if self.print is True:
        print(res)

    self.ssr = best_val

    # obtain the break point locations from the best result
    var = np.sort(best_break)
    breaks = np.zeros(len(var) + 2)
    breaks[1:-1] = var.copy()
    breaks[0] = self.break_0
    breaks[-1] = self.break_n

    # assign parameters
    self.fit_with_breaks(breaks)

    return self.fit_breaks}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.p_values"><code class="name flex">
<span>def <span class="ident">p_values</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the p-values for each beta parameter.</p>
<p>This calculates the p-values for the beta parameters under the
assumption that your break point locations are known. Section 2.4.2 of
[1]_ defines how to calculate the p-value of individual parameters.
This is really a marginal test since each parameter is dependent upon
the other parameters.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>p-values for each beta parameter where p-value[0] corresponds to
beta[0] and so forth</dd>
<dt>Raises</dt>
<dt><strong><code>ValueError</code></strong></dt>
<dd>You have probably not performed a fit yet.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>This assumes that your break point locations are exact! and does
not consider the uncertainty with your break point locations.</p>
<p>See https://github.com/cjekel/piecewise_linear_fit_py/issues/14</p>
<h2 id="references">References</h2>
<p>.. [1] Myers RH, Montgomery DC, Anderson-Cook CM. Response surface
methodology . Hoboken. New Jersey: John Wiley &amp; Sons, Inc.
2009;20:38-44.</p>
<h2 id="examples">Examples</h2>
<p>After performing a fit, one can calculate the p-value for each beta
parameter</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
&gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
&gt;&gt;&gt; p = my_pwlf.p_values(x_new)
</code></pre>
<p>see also examples/standard_errrors_and_p-values.py</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def p_values(self):
    r&#34;&#34;&#34;
    Calculate the p-values for each beta parameter.

    This calculates the p-values for the beta parameters under the
    assumption that your break point locations are known. Section 2.4.2 of
    [1]_ defines how to calculate the p-value of individual parameters.
    This is really a marginal test since each parameter is dependent upon
    the other parameters.

    Returns
    -------
    p : ndarray (1-D)
        p-values for each beta parameter where p-value[0] corresponds to
        beta[0] and so forth

    Raises
    ValueError
        You have probably not performed a fit yet.

    Notes
    -----
    This assumes that your break point locations are exact! and does
    not consider the uncertainty with your break point locations.

    See https://github.com/cjekel/piecewise_linear_fit_py/issues/14

    References
    ----------
    .. [1] Myers RH, Montgomery DC, Anderson-Cook CM. Response surface
        methodology . Hoboken. New Jersey: John Wiley &amp; Sons, Inc.
        2009;20:38-44.

    Examples
    --------
    After performing a fit, one can calculate the p-value for each beta
    parameter

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
    &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
    &gt;&gt;&gt; p = my_pwlf.p_values(x_new)

    see also examples/standard_errrors_and_p-values.py

    &#34;&#34;&#34;
    # calculate the standard errors associated with each beta parameter
    # not that these standard errors and p-values are only meaningful if
    # you have specified the specific line segment end locations
    # at least for now...
    self.standard_errors()

    # calculate my t-value
    t = self.beta / self.se

    # degrees of freedom for t-distribution
    n = self.n_data
    try:
        k = len(self.beta)
    except ValueError:
        errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                 &#39; a fit before using standard_errors().&#39;
        raise ValueError(errmsg)
    # calculate the p-values
    p = stats.t.sf(np.abs(t), df=n-k-1)
    return p}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x, sorted_data=False, beta=None, breaks=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Evaluate the fitted continuous piecewise linear function at untested
points.</p>
<p>You can manfully specify the break points and calculated
values for beta if you want to quickly predict from different models
and the same data set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations where you want to predict the output of the fitted
continuous piecewise linear function.</dd>
<dt><strong><code>sorted_data</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Data needs to be sorted such that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n-1].
This implentation takes advantage of sorted x data in order to
speed up the assembly of the regression matrix. A processes that
could be repeated several thousand times. If your data is not
sorted, pwlf will use numpy to sort the data. Default is False.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>none</code> or <code>ndarray</code> (<code>1</code>-<code>D</code>), optional</dt>
<dd>The model parameters for the continuous piecewise linear fit.
Default is None.</dd>
<dt><strong><code>breaks</code></strong> :&ensp;<code>none</code> or <code>array_like</code>, optional</dt>
<dd>The x locations where each line segment terminates. These are
referred to as break points for each line segment. This should be
structured as a 1-D numpy array. Default is None.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>fit_breaks</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Break point locations stored as a 1-D numpy array.</dd>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The model parameters for the continuous piecewise linear fit.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y_hat</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Returns the Lagrangian function value. This is the sum of squares
of the residuals plus the constraint penalty.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>The above attributes are added or modified if any optional parameter
is specified.</p>
<h2 id="examples">Examples</h2>
<p>Fits a simple model, then predict at x_new locations which are
linearly spaced.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
&gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)
&gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
&gt;&gt;&gt; yhat = my_pwlf.predict(x_new)
</code></pre>
<p>If the x data is already sorted you can add the sorted_data=True to
avoid sorting already sorted data.</p>
<blockquote>
<blockquote>
<blockquote>
<p>yhat = my_pwlf.predict(x_new, sorted_data=False)</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, x, sorted_data=False, beta=None, breaks=None):
    r&#34;&#34;&#34;
    Evaluate the fitted continuous piecewise linear function at untested
    points.

    You can manfully specify the break points and calculated
    values for beta if you want to quickly predict from different models
    and the same data set.

    Parameters
    ----------
    x : array_like
        The x locations where you want to predict the output of the fitted
        continuous piecewise linear function.
    sorted_data : bool, optional
        Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
        This implentation takes advantage of sorted x data in order to
        speed up the assembly of the regression matrix. A processes that
        could be repeated several thousand times. If your data is not
        sorted, pwlf will use numpy to sort the data. Default is False.
    beta : none or ndarray (1-D), optional
        The model parameters for the continuous piecewise linear fit.
        Default is None.
    breaks : none or array_like, optional
        The x locations where each line segment terminates. These are
        referred to as break points for each line segment. This should be
        structured as a 1-D numpy array. Default is None.

    Attributes
    ----------
    fit_breaks : ndarray (1-D)
        Break point locations stored as a 1-D numpy array.
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    n_segments : int
        The number of line segments.
    beta : ndarray (1-D)
        The model parameters for the continuous piecewise linear fit.

    Returns
    -------
    y_hat : ndarray (1-D)
        Returns the Lagrangian function value. This is the sum of squares
        of the residuals plus the constraint penalty.

    Notes
    -----
    The above attributes are added or modified if any optional parameter
    is specified.

    Examples
    -------
    Fits a simple model, then predict at x_new locations which are
    linearly spaced.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = [0.0, 0.3, 0.6, 1.0]
    &gt;&gt;&gt; ssr = my_pwlf.fit_with_breaks(breaks)
    &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
    &gt;&gt;&gt; yhat = my_pwlf.predict(x_new)

    If the x data is already sorted you can add the sorted_data=True to
    avoid sorting already sorted data.

    &gt;&gt;&gt; yhat = my_pwlf.predict(x_new, sorted_data=False)

    &#34;&#34;&#34;
    if beta is not None and breaks is not None:
        self.beta = beta
        # Sort the breaks, then store them
        breaks_order = np.argsort(breaks)
        self.fit_breaks = breaks[breaks_order]
        self.n_parameters = len(self.fit_breaks)
        self.n_segments = self.n_parameters - 1

    # check if x is numpy array, if not convert to numpy array
    if isinstance(x, np.ndarray) is False:
        x = np.array(x)

    # it is assumed by default that initial arrays are not sorted
    # i.e. if your data is already ordered
    # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
    if sorted_data is False:
        # sort the data from least x to max x
        order_arg = np.argsort(x)
        x = x[order_arg]

    # initialize the regression matrix as zeros
    A = np.zeros((len(x), self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = x - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the locations where x &gt; break point values
        int_locations = x &gt; self.fit_breaks[i+1]
        if sum(int_locations) &gt; 0:
            # this if statement just ensures that there is at least
            # one data point in x_c &gt; breaks[i+1]
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(int_locations)
            # only change the non-zero values of A
            A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

    # solve the regression problem
    y_hat = np.dot(A, self.beta)
    return y_hat}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.prediction_variance"><code class="name flex">
<span>def <span class="ident">prediction_variance</span></span>(<span>self, x, sorted_data=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the prediction variance for each specified x location. The
prediction variance is the uncertainty of the model due to the lack of
data. This can be used to find a 95% confidence interval of possible
piecewise linear models based on the current data. This would be
done typically as y_hat +- 1.96*np.sqrt(pre_var). The
prediction_variance needs to be calculated at various x locations.
For more information see:
www2.mae.ufl.edu/haftka/vvuq/lectures/Regression-accuracy.pptx</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>array_like</code></dt>
<dd>The x locations where you want the prediction variance from the
fitted continuous piecewise linear function.</dd>
<dt><strong><code>sorted_data</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Data needs to be sorted such that x[0] &lt;= x[1] &lt;= &hellip; &lt;= x[n-1].
This implentation takes advantage of sorted x data in order to
speed up the assembly of the regression matrix. A processes that
could be repeated several thousand times. If your data is not
sorted, pwlf will use numpy to sort the data. Default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pre_var</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Numpy array (floats) of prediction variance at each x location.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>You have probably not performed a fit yet.</dd>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>This assumes that your break point locations are exact! and does
not consider the uncertainty with your break point locations.</p>
<h2 id="examples">Examples</h2>
<p>Calculate the prediction variance at x_new after performing a simple
fit.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
&gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
&gt;&gt;&gt; pre_var = my_pwlf.prediction_variance(x_new)
</code></pre>
<p>see also examples/prediction_variance.py</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def prediction_variance(self, x, sorted_data=False):
    r&#34;&#34;&#34;
    Calculate the prediction variance for each specified x location. The
    prediction variance is the uncertainty of the model due to the lack of
    data. This can be used to find a 95% confidence interval of possible
    piecewise linear models based on the current data. This would be
    done typically as y_hat +- 1.96*np.sqrt(pre_var). The
    prediction_variance needs to be calculated at various x locations.
    For more information see:
    www2.mae.ufl.edu/haftka/vvuq/lectures/Regression-accuracy.pptx

    Parameters
    ----------
    x : array_like
        The x locations where you want the prediction variance from the
        fitted continuous piecewise linear function.
    sorted_data : bool, optional
        Data needs to be sorted such that x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1].
        This implentation takes advantage of sorted x data in order to
        speed up the assembly of the regression matrix. A processes that
        could be repeated several thousand times. If your data is not
        sorted, pwlf will use numpy to sort the data. Default is False.

    Returns
    -------
    pre_var : ndarray (1-D)
        Numpy array (floats) of prediction variance at each x location.

    Raises
    ------
    ValueError
        You have probably not performed a fit yet.
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    This assumes that your break point locations are exact! and does
    not consider the uncertainty with your break point locations.

    Examples
    --------
    Calculate the prediction variance at x_new after performing a simple
    fit.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
    &gt;&gt;&gt; x_new = np.linspace(0.0, 1.0, 100)
    &gt;&gt;&gt; pre_var = my_pwlf.prediction_variance(x_new)

    see also examples/prediction_variance.py

    &#34;&#34;&#34;
    try:
        nb = len(self.beta)
    except ValueError:
        errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                 &#39; a fit before using standard_errors().&#39;
        raise ValueError(errmsg)

    ny = len(self.y_data)

    # check if x is numpy array, if not convert to numpy array
    if isinstance(x, np.ndarray) is False:
        x = np.array(x)

    # it is assumed by default that initial arrays are not sorted
    # i.e. if your data is already ordered
    # from x[0] &lt;= x[1] &lt;= ... &lt;= x[n-1] use sorted_data=True
    if sorted_data is False:
        # sort the data from least x to max x
        order_arg = np.argsort(x)
        x = x[order_arg]

    # calculate the prediction variance
    Ad = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    Ad[:, 0] = 1.0
    Ad[:, 1] = self.x_data - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
        # only change the non-zero values of A
        Ad[int_index:, i+2] = self.x_data[int_index:] - \
            self.fit_breaks[i+1]

    # try to solve for the unbiased variance estimation
    try:

        y_hat = np.dot(Ad, self.beta)
        e = y_hat - self.y_data

        # solve for the unbiased estimate of variance
        variance = np.dot(e, e) / (ny - nb)

    except np.linalg.LinAlgError:
        raise(&#34;Unable to calculate prediction variance.&#34;
              &#34; Something went wrong.&#34;)

    # initialize the regression matrix as zeros
    A = np.zeros((len(x), self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = x - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the locations where x &gt; break point values
        int_locations = x &gt; self.fit_breaks[i+1]
        if sum(int_locations) &gt; 0:
            # this if statement just ensures that there is at least
            # one data point in x_c &gt; breaks[i+1]
            # find the first index of x where it is greater than the break
            # point value
            int_index = np.argmax(int_locations)
            # only change the non-zero values of A
            A[int_index:, i+2] = x[int_index:] - self.fit_breaks[i+1]

    # try to solve for the prediction variance at the x locations
    try:
        pre_var = variance * \
            np.dot(np.dot(A, np.linalg.inv(np.dot(Ad.T, Ad))), A.T)
        return pre_var.diagonal()

    except np.linalg.LinAlgError:
        raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.r_squared"><code class="name flex">
<span>def <span class="ident">r_squared</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><dl>
<dt>Calculate the coefficient of determination ("R squared", R^2) value</dt>
<dt>after a fit has been performed.</dt>
<dt>For more information see:</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>en.wikipedia.org</code>/<code>wiki</code>/<code>Coefficient_of_determination</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>rsq</code></strong> :&ensp;<code>float</code></dt>
<dd>Coefficient of determination, or 'R squared' value.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>You have probably not performed a fit yet.</dd>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Calculate the R squared value after performing a simple fit.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>rsq = PiecewiseLinFit.r_squared()</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def r_squared(self):
    r&#34;&#34;&#34;
    Calculate the coefficient of determination (&#34;R squared&#34;, R^2) value
    after a fit has been performed.
    For more information see:
    https://en.wikipedia.org/wiki/Coefficient_of_determination

    Returns
    -------
    rsq : float
        Coefficient of determination, or &#39;R squared&#39; value.

    Raises
    ------
    ValueError
        You have probably not performed a fit yet.
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Examples
    --------
    Calculate the R squared value after performing a simple fit.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
    &gt;&gt;&gt; rsq = PiecewiseLinFit.r_squared()

    &#34;&#34;&#34;
    try:
        fit_breaks = self.fit_breaks
    except ValueError:
        errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                 &#39; a fit before using standard_errors().&#39;
        raise ValueError(errmsg)
    ssr = self.fit_with_breaks(fit_breaks)
    ybar = np.ones(self.n_data) * np.mean(self.y_data)
    ydiff = self.y_data - ybar
    try:
        sst = np.dot(ydiff, ydiff)
        rsq = 1.0 - (ssr/sst)
        return rsq
    except np.linalg.LinAlgError:
        raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.standard_errors"><code class="name flex">
<span>def <span class="ident">standard_errors</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><dl>
<dt>Calculate the standard errors for each beta parameter determined</dt>
<dt>from the piecewise linear fit. Typically +- 1.96*se will yield the</dt>
<dt>center of a 95% confidence region around your parameters. This</dt>
<dt>assumes the parmaters follow a normal distribution. For more</dt>
<dt>information see:</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>en.wikipedia.org</code>/<code>wiki</code>/<code>Standard_error</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>This calculation follows the derivation provided in [1]_. A taylor-
series expansion is not needed since this is linear regression.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>se</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>Standard errors associated with each beta parameter. Specifically
se[0] correspounds to the standard error for beta[0], and so forth.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>ValueError</code></strong></dt>
<dd>You have probably not performed a fit yet.</dd>
<dt><strong><code>LinAlgError</code></strong></dt>
<dd>This typically means your regression problem is ill-conditioned.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Note, this assumes no uncertainty in break point locations.</p>
<h2 id="references">References</h2>
<p>.. [1] Coppe, A., Haftka, R. T., and Kim, N. H., “Uncertainty
Identification of Damage Growth Parameters Using Nonlinear
Regression,” AIAA Journal, Vol. 49, No. 12, dec 2011, pp.
2818–2821.</p>
<h2 id="examples">Examples</h2>
<p>Calculate the standard errors after performing a simple fit.</p>
<pre><code>&gt;&gt;&gt; import pwlf
&gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
&gt;&gt;&gt; y = np.random.random(10)
&gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
&gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<p>se = my_pwlf.standard_errors()</p>
</blockquote>
</blockquote>
</blockquote></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def standard_errors(self):
    r&#34;&#34;&#34;
    Calculate the standard errors for each beta parameter determined
    from the piecewise linear fit. Typically +- 1.96*se will yield the
    center of a 95% confidence region around your parameters. This
    assumes the parmaters follow a normal distribution. For more
    information see:
    https://en.wikipedia.org/wiki/Standard_error

    This calculation follows the derivation provided in [1]_. A taylor-
    series expansion is not needed since this is linear regression.

    Returns
    -------
    se : ndarray (1-D)
        Standard errors associated with each beta parameter. Specifically
        se[0] correspounds to the standard error for beta[0], and so forth.

    Raises
    ------
    ValueError
        You have probably not performed a fit yet.
    LinAlgError
        This typically means your regression problem is ill-conditioned.

    Notes
    -----
    Note, this assumes no uncertainty in break point locations.

    References
    ----------
    .. [1] Coppe, A., Haftka, R. T., and Kim, N. H., “Uncertainty
        Identification of Damage Growth Parameters Using Nonlinear
        Regression,” AIAA Journal, Vol. 49, No. 12, dec 2011, pp.
        2818–2821.

    Examples
    --------
    Calculate the standard errors after performing a simple fit.

    &gt;&gt;&gt; import pwlf
    &gt;&gt;&gt; x = np.linspace(0.0, 1.0, 10)
    &gt;&gt;&gt; y = np.random.random(10)
    &gt;&gt;&gt; my_pwlf = pwlf.PiecewiseLinFit(x, y)
    &gt;&gt;&gt; breaks = my_pwlf.fitfast(3)
    &gt;&gt;&gt; se = my_pwlf.standard_errors()

    &#34;&#34;&#34;
    try:
        nb = len(self.beta)
    except ValueError:
        errmsg = &#39;You do not have any beta parameters. You must perform&#39; \
                 &#39; a fit before using standard_errors().&#39;
        raise ValueError(errmsg)

    ny = len(self.y_data)

    # initialize the regression matrix as zeros
    A = np.zeros((self.n_data, self.n_parameters))
    # The first two columns of the matrix are always defined as
    A[:, 0] = 1.0
    A[:, 1] = self.x_data - self.fit_breaks[0]
    # Loop through the rest of A to determine the other columns
    for i in range(self.n_segments-1):
        # find the first index of x where it is greater than the break
        # point value
        int_index = np.argmax(self.x_data &gt; self.fit_breaks[i+1])
        # only change the non-zero values of A
        A[int_index:, i+2] = self.x_data[int_index:] - self.fit_breaks[i+1]

    # try to solve for the standard errors
    try:

        y_hat = np.dot(A, self.beta)
        e = y_hat - self.y_data

        # solve for the unbiased estimate of variance
        variance = np.dot(e, e) / (ny - nb)

        self.se = np.sqrt(variance * (np.linalg.inv(np.dot(A.T,
                                                           A)).diagonal()))

        return self.se

    except np.linalg.LinAlgError:
        raise(&#39;Unable to calculate standard errors. Something went wrong.&#39;)}</code></pre>
</details>
</dd>
<dt id="pwlf.pwlf.PiecewiseLinFit.use_custom_opt"><code class="name flex">
<span>def <span class="ident">use_custom_opt</span></span>(<span>self, n_segments, x_c=None, y_c=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Provide the number of line segments you want to use with your
custom optimization routine.</p>
<p>Run this function first to initialize necessary attributes!!!</p>
<p>This was intended for advanced users only.</p>
<dl>
<dt>See the following example</dt>
<dt><strong><code>https</code></strong> :&ensp;//<code>github.com</code>/<code>cjekel</code>/<code>piecewise_linear_fit_py</code>/<code>blob</code>/<code>master</code>/<code>examples</code>/<code>useCustomOptimizationRoutine.py</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The x locations where each line segment terminates. These are
referred to as break points for each line segment. This should be
structured as a 1-D numpy array.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>none</code> or <code>array_like</code>, optional</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>none</code> or <code>array_like</code>, optional</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of model parameters. This is equivalent to the
len(beta).</dd>
<dt><strong><code>nVar</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of variables in the global optimization problem.</dd>
<dt><strong><code>n_segments</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of line segments.</dd>
<dt><strong><code>x_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>y_c</code></strong> :&ensp;<code>ndarray</code> (<code>1</code>-<code>D</code>)</dt>
<dd>The x locations of the data points that the piecewise linear
function will be forced to go through.</dd>
<dt><strong><code>c_n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of constraint points. This is the same as len(x_c).</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Optimize fit_with_breaks_opt(var) where var is a 1D array
containing the x locations of your variables
var has length n_segments - 1, because the two break points
are always defined (1. the min of x, 2. the max of x).</p>
<p>fit_with_breaks_opt(var) will return the sum of the square of the
residuals which you'll want to minimize with your optimization
routine.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def use_custom_opt(self, n_segments, x_c=None, y_c=None):
    r&#34;&#34;&#34;
    Provide the number of line segments you want to use with your
    custom optimization routine.

    Run this function first to initialize necessary attributes!!!

    This was intended for advanced users only.

    See the following example
    https://github.com/cjekel/piecewise_linear_fit_py/blob/master/examples/useCustomOptimizationRoutine.py

    Parameters
    ----------
    n_segments : int
        The x locations where each line segment terminates. These are
        referred to as break points for each line segment. This should be
        structured as a 1-D numpy array.
    x_c : none or array_like, optional
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    y_c : none or array_like, optional
        The x locations of the data points that the piecewise linear
        function will be forced to go through.

    Attributes
    ----------
    n_parameters : int
        The number of model parameters. This is equivalent to the
        len(beta).
    nVar : int
        The number of variables in the global optimization problem.
    n_segments : int
        The number of line segments.
    x_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    y_c : ndarray (1-D)
        The x locations of the data points that the piecewise linear
        function will be forced to go through.
    c_n : int
        The number of constraint points. This is the same as len(x_c).

    Notes
    -----
    Optimize fit_with_breaks_opt(var) where var is a 1D array
    containing the x locations of your variables
    var has length n_segments - 1, because the two break points
    are always defined (1. the min of x, 2. the max of x).

    fit_with_breaks_opt(var) will return the sum of the square of the
    residuals which you&#39;ll want to minimize with your optimization
    routine.
    &#34;&#34;&#34;

    self.n_segments = int(n_segments)
    self.n_parameters = self.n_segments + 1

    # calculate the number of variables I have to solve for
    self.nVar = self.n_segments - 1
    if x_c is not None or y_c is not None:
        # check if x_c and y_c are numpy array
        # if not convert to numpy array
        if isinstance(x_c, np.ndarray) is False:
            x_c = np.array(x_c)
        if isinstance(y_c, np.ndarray) is False:
            y_c = np.array(y_c)
        # sort the x_c and y_c data points, then store them
        x_c_order = np.argsort(x_c)
        self.x_c = x_c[x_c_order]
        self.y_c = y_c[x_c_order]
        # store the number of constraints
        self.c_n = len(self.x_c)}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pwlf" href="..\index.html">pwlf</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pwlf.pwlf.PiecewiseLinFit" href="#pwlf.pwlf.PiecewiseLinFit">PiecewiseLinFit</a></code></h4>
<ul class="two-column">
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.__init__" href="#pwlf.pwlf.PiecewiseLinFit.__init__">__init__</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.calc_slopes" href="#pwlf.pwlf.PiecewiseLinFit.calc_slopes">calc_slopes</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fit" href="#pwlf.pwlf.PiecewiseLinFit.fit">fit</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fit_force_points_opt" href="#pwlf.pwlf.PiecewiseLinFit.fit_force_points_opt">fit_force_points_opt</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks" href="#pwlf.pwlf.PiecewiseLinFit.fit_with_breaks">fit_with_breaks</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_force_points" href="#pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_force_points">fit_with_breaks_force_points</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_opt" href="#pwlf.pwlf.PiecewiseLinFit.fit_with_breaks_opt">fit_with_breaks_opt</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.fitfast" href="#pwlf.pwlf.PiecewiseLinFit.fitfast">fitfast</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.p_values" href="#pwlf.pwlf.PiecewiseLinFit.p_values">p_values</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.predict" href="#pwlf.pwlf.PiecewiseLinFit.predict">predict</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.prediction_variance" href="#pwlf.pwlf.PiecewiseLinFit.prediction_variance">prediction_variance</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.r_squared" href="#pwlf.pwlf.PiecewiseLinFit.r_squared">r_squared</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.standard_errors" href="#pwlf.pwlf.PiecewiseLinFit.standard_errors">standard_errors</a></code></li>
<li><code><a title="pwlf.pwlf.PiecewiseLinFit.use_custom_opt" href="#pwlf.pwlf.PiecewiseLinFit.use_custom_opt">use_custom_opt</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>